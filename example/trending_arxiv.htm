<!doctype html>
<html lang="en">
<head>
  <title>Trending arXiv</title>
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
  <meta name="viewport" content="width=device-width, initial-scale=1">
</head>
<body>

  <a href="https://github.com/Smerity/trending_arxiv"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/38ef81f8aca64bb9a64448d0d70f1308ef5341ab/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"></a>

  <div class="container">
  <h1>
    <a href="/refresh" title="Refresh"><span class="glyphicon glyphicon-refresh" aria-hidden="true"></span></a>
    Trending arXiv
  </h1>
  <ul class="nav nav-pills">
    <li role="presentation"><a href="/"><span aria-hidden="true" class="glyphicon glyphicon-file"></span> Papers</a></li>
    <li role="presentation"><a href="/tweets"><span aria-hidden="true" class="glyphicon glyphicon-comment"></span> Tweets</a></li>
    <li role="presentation"><a href="/rate_limits"><span class="glyphicon glyphicon-signal" aria-hidden="true"></span> Rate Limits</a></li>
    <li role="presentation"><a href="/refresh"><span class="glyphicon glyphicon-refresh" aria-hidden="true"></span> Refresh</a></li>
  </ul>
  
    
  
  
  <h3>Papers</h3>
  <hr />

  <span class="glyphicon glyphicon-chevron-left" aria-hidden="true"></span> Newer papers |
  <a href="/2">Older papers <span class="glyphicon glyphicon-chevron-right" aria-hidden="true"></span></a>
    

  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1609.02353" target="_blank">9-1-1 DDoS: Threat, Analysis and Mitigation</a> <a href="http://arxiv.org/pdf/1609.02353" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Mordechai Guri, Yisroel Mirsky, Yuval Elovici</i>
    <p>  The 911 emergency service belongs to one of the 16 critical infrastructure
sectors in the United States. Distributed denial of service (DDoS) attacks
launched from a mobile phone botnet pose a significant threat to the
availability of this vital service. In this paper we show how attackers can
exploit the cellular network protocols in order to launch an anonymized DDoS
attack on 911. The current FCC regulations require that all emergency calls be
immediately routed regardless of the caller&#39;s identifiers (e.g., IMSI and
IMEI). A rootkit placed within the baseband firmware of a mobile phone can mask
and randomize all cellular identifiers, causing the device to have no genuine
identification within the cellular network. Such anonymized phones can issue
repeated emergency calls that cannot be blocked by the network or the emergency
call centers, technically or legally. We explore the 911 infrastructure and
discuss why it is susceptible to this kind of attack. We then implement
different forms of the attack and test our implementation on a small cellular
network. Finally, we simulate and analyze anonymous attacks on a model of
current 911 infrastructure in order to measure the severity of their impact. We
found that with less than 6K bots (or $100K hardware), attackers can block
emergency services in an entire state (e.g., North Carolina) for days. We
believe that this paper will assist the respective organizations, lawmakers,
and security professionals in understanding the scope of this issue in order to
prevent possible 911-DDoS attacks in the future.
</p>
    <p>Captured tweets and retweets: 10</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/smerity/status/774763166194475009"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1609.02087" target="_blank">Clearing the Skies: A deep network architecture for single-image rain
  removal</a> <a href="http://arxiv.org/pdf/1609.02087" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Xueyang Fu, Jiabin Huang, Xinghao Ding, Yinghao Liao, John Paisley</i>
    <p>  We introduce a deep network architecture called DerainNet for removing rain
streaks from an image. Based on the deep convolutional neural network (CNN), we
directly learn the mapping relationship between rainy and clean image detail
layers from data. Because we do not possess the ground truth corresponding to
real-world rainy images, we synthesize images with rain for training. To
effectively and efficiently train the network, different with common strategies
that roughly increase depth or breadth of network, we utilize some image
processing domain knowledge to modify the objective function. Specifically, we
train our DerainNet on the detail layer rather than the image domain. Better
results can be obtained under the same net architecture. Though DerainNet is
trained on synthetic data, we still find that the learned network is very
effective on real-world images for testing. Moreover, we augment the CNN
framework with image enhancement to significantly improve the visual results.
Compared with state-of-the- art single image de-rain methods, our method has
better rain removal and much faster computation time after network training.
</p>
    <p>Captured tweets and retweets: 12</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/alexjc/status/773791610375143424"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1609.01704" target="_blank">Hierarchical Multiscale Recurrent Neural Networks</a> <a href="http://arxiv.org/pdf/1609.01704" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Junyoung Chung, Sungjin Ahn, Yoshua Bengio</i>
    <p>  Learning both hierarchical and temporal representation has been among the
long-standing challenges of recurrent neural networks. Multiscale recurrent
neural networks have been considered as a promising approach to resolve this
issue, yet there has been a lack of empirical evidence showing that this type
of models can actually capture the temporal dependencies by discovering the
latent hierarchical structure of the sequence. In this paper, we propose a
novel multiscale approach, called the hierarchical multiscale recurrent neural
networks, which can capture the latent hierarchical structure in the sequence
by encoding the temporal dependencies with different timescales using a novel
update mechanism. We show some evidence that our proposed multiscale
architecture can discover underlying hierarchical structure in the sequences
without using explicit boundary information. We evaluate our proposed model on
character-level language modelling and handwriting sequence modelling.
</p>
    <p>Captured tweets and retweets: 73</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/nandodf/status/773422416571883524"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/kastnerkyle/status/773414482899046401"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1609.00408" target="_blank">Defeating Image Obfuscation with Deep Learning</a> <a href="http://arxiv.org/pdf/1609.00408" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Richard McPherson, Reza Shokri, Vitaly Shmatikov</i>
    <p>  We demonstrate that modern image recognition methods based on artificial
neural networks can recover hidden information from images protected by various
forms of obfuscation. The obfuscation techniques considered in this paper are
mosaicing (also known as pixelation), blurring (as used by YouTube), and P3, a
recently proposed system for privacy-preserving photo sharing that encrypts the
significant JPEG coefficients to make images unrecognizable by humans. We
empirically show how to train artificial neural networks to successfully
identify faces and recognize objects and handwritten digits even if the images
are protected using any of the above obfuscation techniques.
</p>
    <p>Captured tweets and retweets: 78</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/alexjc/status/772721415317585920"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1609.00150" target="_blank">Reward Augmented Maximum Likelihood for Neural Structured Prediction</a> <a href="http://arxiv.org/pdf/1609.00150" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Mohammad Norouzi, Samy Bengio, Zhifeng Chen, Navdeep Jaitly, Mike Schuster, Yonghui Wu, Dale Schuurmans</i>
    <p>  A key problem in structured output prediction is direct optimization of the
task reward function that matters for test evaluation. This paper presents a
simple and computationally efficient approach to incorporate task reward into a
maximum likelihood framework. We establish a connection between the
log-likelihood and regularized expected reward objectives, showing that at a
zero temperature, they are approximately equivalent in the vicinity of the
optimal solution. We show that optimal regularized expected reward is achieved
when the conditional distribution of the outputs given the inputs is
proportional to their exponentiated (temperature adjusted) rewards. Based on
this observation, we optimize conditional log-probability of edited outputs
that are sampled proportionally to their scaled exponentiated reward. We apply
this framework to optimize edit distance in the output label space. Experiments
on speech recognition and machine translation for neural sequence to sequence
models show notable improvements over a maximum likelihood baseline by using
edit distance augmented maximum likelihood.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/j_gauthier/status/773364397192261633"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1609.00037" target="_blank">Good Enough Practices in Scientific Computing</a> <a href="http://arxiv.org/pdf/1609.00037" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Greg Wilson, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, Tracy K. Teal</i>
    <p>  We present a set of computing tools and techniques that every researcher can
and should adopt. These recommendations synthesize inspiration from our own
work, from the experiences of the thousands of people who have taken part in
Software Carpentry and Data Carpentry workshops over the past six years, and
from a variety of other guides. Unlike some other guides, our recommendations
are aimed specifically at people who are new to research computing.
</p>
    <p>Captured tweets and retweets: 23</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/deliprao/status/773221533846122496"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.08868" target="_blank">Demographic Dialectal Variation in Social Media: A Case Study of
  African-American English</a> <a href="http://arxiv.org/pdf/1608.08868" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Su Lin Blodgett, Lisa Green, Brendan O&#39;Connor</i>
    <p>  Though dialectal language is increasingly abundant on social media, few
resources exist for developing NLP tools to handle such language. We conduct a
case study of dialectal language in online conversational text by investigating
African-American English (AAE) on Twitter. We propose a distantly supervised
model to identify AAE-like language from demographics associated with
geo-located messages, and we verify that this language follows well-known AAE
linguistic phenomena. In addition, we analyze the quality of existing language
identification and dependency parsing tools on AAE-like text, demonstrating
that they perform poorly on such text compared to text associated with white
speakers. We also provide an ensemble classifier for language identification
which eliminates this disparity and release a new corpus of tweets containing
AAE-like language.
</p>
    <p>Captured tweets and retweets: 4</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/stanfordnlp/status/773535701841260544"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/brendan642/status/773933656050130949"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.08225" target="_blank">Why does deep and cheap learning work so well?</a> <a href="http://arxiv.org/pdf/1608.08225" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Henry W. Lin, Max Tegmark</i>
    <p>  We show how the success of deep learning depends not only on mathematics but
also on physics: although well-known mathematical theorems guarantee that
neural networks can approximate arbitrary functions well, the class of
functions of practical interest can be approximated through &#34;cheap learning&#34;
with exponentially fewer parameters than generic ones, because they have
simplifying properties tracing back to the laws of physics. The exceptional
simplicity of physics-based functions hinges on properties such as symmetry,
locality, compositionality and polynomial log-probability, and we explore how
these properties translate into exceptionally simple neural networks
approximating both natural phenomena such as images and abstract
representations thereof such as drawings. We further argue that when the
statistical process generating the data is of a certain hierarchical form
prevalent in physics and machine-learning, a deep neural network can be more
efficient than a shallow one. We formalize these claims using information
theory and discuss the relation to renormalization group procedures. Various
&#34;no-flattening theorems&#34; show when these efficient deep networks cannot be
accurately approximated by shallow ones without efficiency loss - even for
linear networks.
</p>
    <p>Captured tweets and retweets: 8</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/deliprao/status/770788372491210752"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/petewarden/status/771759924095377408"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/janexwang/status/771800321127505920"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.07905" target="_blank">Machine Comprehension Using Match-LSTM and Answer Pointer</a> <a href="http://arxiv.org/pdf/1608.07905" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Shuohang Wang, Jing Jiang</i>
    <p>  Machine comprehension of text is an important problem in natural language
processing. A recently released dataset, the Stanford Question Answering
Dataset (SQuAD), offers a large number of real questions and their answers
created by humans through crowdsourcing. SQuAD provides a challenging testbed
for evaluating machine comprehension algorithms, partly because compared with
previous datasets, in SQuAD the answers do not come from a small set of
candidate answers and they have variable lengths. We propose an end-to-end
neural architecture for the task. The architecture is based on match-LSTM, a
model we proposed previously for textual entailment, and Pointer Net, a
sequence-to-sequence model proposed by Vinyals et al.(2015) to constrain the
output tokens to be from the input sequences. We propose two ways of using
Pointer Net for our task. Our experiments show that both of our two models
substantially outperform the best results obtained by Rajpurkar et al.(2016)
using logistic regression and manually crafted features.
</p>
    <p>Captured tweets and retweets: 20</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/fchollet/status/770750317596119041"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/stanfordnlp/status/772445357489819648"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.07068" target="_blank">Title Generation for User Generated Videos</a> <a href="http://arxiv.org/pdf/1608.07068" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Kuo-Hao Zeng, Tseng-Hung Chen, Juan Carlos Niebles, Min Sun</i>
    <p>  A great video title describes the most salient event compactly and captures
the viewer&#39;s attention. In contrast, video captioning tends to generate
sentences that describe the video as a whole. Although generating a video title
automatically is a very useful task, it is much less addressed than video
captioning. We address video title generation for the first time by proposing
two methods that extend state-of-the-art video captioners to this new task.
First, we make video captioners highlight sensitive by priming them with a
highlight detector. Our framework allows for jointly training a model for title
generation and video highlight localization. Second, we induce high sentence
diversity in video captioners, so that the generated titles are also diverse
and catchy. This means that a large number of sentences might be required to
learn the sentence structure of titles. Hence, we propose a novel sentence
augmentation method to train a captioner with additional sentence-only examples
that come without corresponding videos. We collected a large-scale Video Titles
in the Wild (VTW) dataset of 18100 automatically crawled user-generated videos
and titles. On VTW, our methods consistently improve title prediction accuracy,
and achieve the best performance in both automatic and human evaluation.
Finally, our sentence augmentation method also outperforms the baselines on the
M-VAD dataset.
</p>
    <p>Captured tweets and retweets: 3</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/alexjc/status/769076592396083202"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.06197" target="_blank">CrowdNet: A Deep Convolutional Network for Dense Crowd Counting</a> <a href="http://arxiv.org/pdf/1608.06197" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Lokesh Boominathan, Srinivas S S Kruthiventi, R. Venkatesh Babu</i>
    <p>  Our work proposes a novel deep learning framework for estimating crowd
density from static images of highly dense crowds. We use a combination of deep
and shallow, fully convolutional networks to predict the density map for a
given crowd image. Such a combination is used for effectively capturing both
the high-level semantic information (face/body detectors) and the low-level
features (blob detectors), that are necessary for crowd counting under large
scale variations. As most crowd datasets have limited training samples (&lt;100
images) and deep learning based approaches require large amounts of training
data, we perform multi-scale data augmentation. Augmenting the training samples
in such a manner helps in guiding the CNN to learn scale invariant
representations. Our method is tested on the challenging UCF_CC_50 dataset, and
shown to outperform the state of the art methods.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/deep_hub/status/768163257488121856"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.05343" target="_blank">Decoupled Neural Interfaces using Synthetic Gradients</a> <a href="http://arxiv.org/pdf/1608.05343" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Max Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, Koray Kavukcuoglu</i>
    <p>  Training directed neural networks typically requires forward-propagating data
through a computation graph, followed by backpropagating error signal, to
produce weight updates. All layers, or more generally, modules, of the network
are therefore locked, in the sense that they must wait for the remainder of the
network to execute forwards and propagate error backwards before they can be
updated. In this work we break this constraint by decoupling modules by
introducing a model of the future computation of the network graph. These
models predict what the result of the modelled subgraph will produce using only
local information. In particular we focus on modelling error gradients: by
using the modelled synthetic gradient in place of true backpropagated error
gradients we decouple subgraphs, and can update them independently and
asynchronously i.e. we realise decoupled neural interfaces. We show results for
feed-forward models, where every layer is trained asynchronously, recurrent
neural networks (RNNs) where predicting one&#39;s future gradient extends the time
over which the RNN can effectively model, and also a hierarchical RNN system
with ticking at different timescales. Finally, we demonstrate that in addition
to predicting gradients, the same framework can be used to predict inputs,
resulting in models which are decoupled in both the forward and backwards pass
-- amounting to independent networks which co-learn such that they can be
composed into a single functioning corporation.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/petewarden/status/766477174660476928"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.05148" target="_blank">Full Resolution Image Compression with Recurrent Neural Networks</a> <a href="http://arxiv.org/pdf/1608.05148" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>George Toderici, Damien Vincent, Nick Johnston, Sung Jin Hwang, David Minnen, Joel Shor, Michele Covell</i>
    <p>  This paper presents a set of full-resolution lossy image compression methods
based on neural networks. Each of the architectures we describe can provide
variable compression rates during deployment without requiring retraining of
the network: each network need only be trained once. All of our architectures
consist of a recurrent neural network (RNN)-based encoder and decoder, a
binarizer, and a neural network for entropy coding. We compare RNN types (LSTM,
associative LSTM) and introduce a new hybrid of GRU and ResNet. We also study
&#34;one-shot&#34; versus additive reconstruction architectures and introduce a new
scaled-additive framework. We compare to previous work, showing improvements of
4.3%-8.8% AUC (area under the rate-distortion curve), depending on the
perceptual metric used. As far as we know, this is the first neural network
architecture that is able to outperform JPEG at image compression across most
bitrates on the rate-distortion curve on the Kodak dataset images, with and
without the aid of entropy coding.
</p>
    <p>Captured tweets and retweets: 3</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/alexjc/status/766611397627219968"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/petewarden/status/768110248091537409"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.05081" target="_blank">Efficient Exploration for Dialog Policy Learning with Deep BBQ Networks
  \&amp; Replay Buffer Spiking</a> <a href="http://arxiv.org/pdf/1608.05081" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Zachary C. Lipton, Jianfeng Gao, Lihong Li, Xiujun Li, Faisal Ahmed, Li Deng</i>
    <p>  When rewards are sparse and efficient exploration essential, deep Q-learning
with $\epsilon$-greedy exploration tends to fail. This poses problems for
otherwise promising domains such as task-oriented dialog systems, where the
primary reward signal, indicating successful completion, typically occurs only
at the end of each episode but depends on the entire sequence of utterances. A
poor agent encounters such successful dialogs rarely, and a random agent may
never stumble upon a successful outcome in reasonable time. We present two
techniques that significantly improve the efficiency of exploration for deep
Q-learning agents in dialog systems. First, we demonstrate that exploration by
Thompson sampling, using Monte Carlo samples from a Bayes-by-Backprop neural
network, yields marked improvement over standard DQNs with Boltzmann or
$\epsilon$-greedy exploration. Second, we show that spiking the replay buffer
with a small number of successes, as are easy to harvest for dialog tasks, can
make Q-learning feasible when it might otherwise fail catastrophically.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/boredyannlecun/status/766908552552132608"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.04631" target="_blank">Neural versus Phrase-Based Machine Translation Quality: a Case Study</a> <a href="http://arxiv.org/pdf/1608.04631" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Luisa Bentivogli, Arianna Bisazza, Mauro Cettolo, Marcello Federico</i>
    <p>  Within the field of Statistical Machine Translation (SMT), the neural
approach (NMT) has recently emerged as the first technology able to challenge
the long-standing dominance of phrase-based approaches (PBMT). In particular,
at the IWSLT 2015 evaluation campaign, NMT outperformed well established
state-of-the-art PBMT systems on English-German, a language pair known to be
particularly hard because of morphology and syntactic differences. To
understand in what respects NMT provides better translation quality than PBMT,
we perform a detailed analysis of neural versus phrase-based SMT outputs,
leveraging high quality post-edits performed by professional translators on the
IWSLT data. For the first time, our analysis provides useful insights on what
linguistic phenomena are best modeled by neural models -- such as the
reordering of verbs -- while pointing out other aspects that remain to be
improved.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/stanfordnlp/status/770705158451793920"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.04428" target="_blank">TerpreT: A Probabilistic Programming Language for Program Induction</a> <a href="http://arxiv.org/pdf/1608.04428" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Alexander L. Gaunt, Marc Brockschmidt, Rishabh Singh, Nate Kushman, Pushmeet Kohli, Jonathan Taylor, Daniel Tarlow</i>
    <p>  We study machine learning formulations of inductive program synthesis; given
input-output examples, we try to synthesize source code that maps inputs to
corresponding outputs. Our aims are to develop new machine learning approaches
based on neural networks and graphical models, and to understand the
capabilities of machine learning techniques relative to traditional
alternatives, such as those based on constraint solving from the programming
languages community.
  Our key contribution is the proposal of TerpreT, a domain-specific language
for expressing program synthesis problems. TerpreT is similar to a
probabilistic programming language: a model is composed of a specification of a
program representation (declarations of random variables) and an interpreter
describing how programs map inputs to outputs (a model connecting unknowns to
observations). The inference task is to observe a set of input-output examples
and infer the underlying program. TerpreT has two main benefits. First, it
enables rapid exploration of a range of domains, program representations, and
interpreter models. Second, it separates the model specification from the
inference algorithm, allowing like-to-like comparisons between different
approaches to inference. From a single TerpreT specification we automatically
perform inference using four different back-ends. These are based on gradient
descent, linear program (LP) relaxations for graphical models, discrete
satisfiability solving, and the Sketch program synthesis system.
  We illustrate the value of TerpreT by developing several interpreter models
and performing an empirical comparison between alternative inference
algorithms. Our key empirical finding is that constraint solvers dominate the
gradient descent and LP-based formulations. We conclude with suggestions for
the machine learning community to make progress on program synthesis.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/numbercrunching/status/766027803338928129"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.04062" target="_blank">Stacked Approximated Regression Machine: A Simple Deep Learning Approach</a> <a href="http://arxiv.org/pdf/1608.04062" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Zhangyang Wang, Shiyu Chang, Qing Ling, Shuai Huang, Xia Hu, Honghui Shi, Thomas S. Huang</i>
    <p>  With the agreement of my coauthors, I Zhangyang Wang would like to withdraw
the manuscript “Stacked Approximated Regression Machine: A Simple Deep Learning
Approach”. Some experimental procedures were not included in the manuscript,
which makes a part of important claims not meaningful. In the relevant
research, I was solely responsible for carrying out the experiments; the other
coauthors joined in the discussions leading to the main algorithm.
  Please see the updated text for more details.
</p>
    <p>Captured tweets and retweets: 89</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/deliprao/status/774301347021586432"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/fchollet/status/774065138592690176"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/randal_olson/status/774276161270448132"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/deep_hub/status/771846438795882496"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/fchollet/status/771862837819867136"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/graycrawford/status/771844248929009664"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/karpathy/status/771822553903923202"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/tim_zaman/status/772029237855416320"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/dennybritz/status/774067398307553280"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.03983" target="_blank">SGDR: Stochastic Gradient Descent with Restarts</a> <a href="http://arxiv.org/pdf/1608.03983" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Ilya Loshchilov, Frank Hutter</i>
    <p>  Restart techniques are common in gradient-free optimization to deal with
multimodal functions. Partial restarts are also gaining popularity in
gradient-based optimization to improve the rate of convergence in accelerated
gradient schemes to deal with ill-conditioned functions. In this paper, we
propose a simple restart technique for stochastic gradient descent to improve
its anytime performance when training deep neural networks. We empirically
study its performance on CIFAR-10 and CIFAR-100 datasets where we demonstrate
new state-of-the-art results below 4\% and 19\%, respectively. Our source code
is available at https://github.com/loshchil/SGDR.
</p>
    <p>Captured tweets and retweets: 5</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/alexjc/status/766236305055514625"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.03819" target="_blank">DeepDiary: Automatic Caption Generation for Lifelogging Image Streams</a> <a href="http://arxiv.org/pdf/1608.03819" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Chenyou Fan, David J. Crandall</i>
    <p>  Lifelogging cameras capture everyday life from a first-person perspective,
but generate so much data that it is hard for users to browse and organize
their image collections effectively. In this paper, we propose to use automatic
image captioning algorithms to generate textual representations of these
collections. We develop and explore novel techniques based on deep learning to
generate captions for both individual images and image streams, using temporal
consistency constraints to create summaries that are both more compact and less
noisy. We evaluate our techniques with quantitative and qualitative results,
and apply captioning to an image retrieval application for finding potentially
private images. Our results suggest that our automatic captioning algorithms,
while imperfect, may work well enough to help users manage lifelogging photo
collections.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/alexjc/status/765118769849069568"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.03785" target="_blank">Compositional Distributional Cognition</a> <a href="http://arxiv.org/pdf/1608.03785" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Yaared Al-Mehairi, Bob Coecke, Martha Lewis</i>
    <p>  We accommodate the Integrated Connectionist/Symbolic Architecture (ICS) of
[32] within the categorical compositional semantics (CatCo) of [13], forming a
model of categorical compositional cognition (CatCog). This resolves intrinsic
problems with ICS such as the fact that representations inhabit an unbounded
space and that sentences with differing tree structures cannot be directly
compared. We do so in a way that makes the most of the grammatical structure
available, in contrast to strategies like circular convolution. Using the CatCo
model also allows us to make use of tools developed for CatCo such as the
representation of ambiguity and logical reasoning via density matrices,
structural meanings for words such as relative pronouns, and addressing over-
and under-extension, all of which are present in cognitive processes. Moreover
the CatCog framework is sufficiently flexible to allow for entirely different
representations of meaning, such as conceptual spaces. Interestingly, since the
CatCo model was largely inspired by categorical quantum mechanics, so is
CatCog.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/miles_brundage/status/764989658430341120"></a></blockquote></center>
    
  </div>
  <hr />
  

  <span class="glyphicon glyphicon-chevron-left" aria-hidden="true"></span> Newer papers |
  <a href="/2">Older papers <span class="glyphicon glyphicon-chevron-right" aria-hidden="true"></span></a>
    

  <hr />

  </div>
  <footer class="footer">
    <div class="container">
      <center>
        <p class="text-muted"><em>Made with a human heart <span class="glyphicon glyphicon-heart-empty" aria-hidden="true"></span> + one part enriched uranium <span class="glyphicon glyphicon-warning-sign" aria-hidden="true"></span> + four parts unicorn blood <span class="glyphicon glyphicon-knight" aria-hidden="true"></span></em></p>
      </center>
    </div>
  </footer>
</body>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
</html>