<!doctype html>
<html lang="en">
<head>
  <title>Trending arXiv</title>
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
  <meta name="viewport" content="width=device-width, initial-scale=1">
</head>
<body>
  <div class="container">
  <h1>
    <a href="/refresh" title="Refresh"><span class="glyphicon glyphicon-refresh" aria-hidden="true"></span></a>
    Trending arXiv
  </h1>
  
    
  
  
  <h3>Papers</h3>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1609.02353" target="_blank">9-1-1 DDoS: Threat, Analysis and Mitigation</a></h4>
    <i>Mordechai Guri, Yisroel Mirsky, Yuval Elovici</i>
    <p>  The 911 emergency service belongs to one of the 16 critical infrastructure
sectors in the United States. Distributed denial of service (DDoS) attacks
launched from a mobile phone botnet pose a significant threat to the
availability of this vital service. In this paper we show how attackers can
exploit the cellular network protocols in order to launch an anonymized DDoS
attack on 911. The current FCC regulations require that all emergency calls be
immediately routed regardless of the caller&#39;s identifiers (e.g., IMSI and
IMEI). A rootkit placed within the baseband firmware of a mobile phone can mask
and randomize all cellular identifiers, causing the device to have no genuine
identification within the cellular network. Such anonymized phones can issue
repeated emergency calls that cannot be blocked by the network or the emergency
call centers, technically or legally. We explore the 911 infrastructure and
discuss why it is susceptible to this kind of attack. We then implement
different forms of the attack and test our implementation on a small cellular
network. Finally, we simulate and analyze anonymous attacks on a model of
current 911 infrastructure in order to measure the severity of their impact. We
found that with less than 6K bots (or $100K hardware), attackers can block
emergency services in an entire state (e.g., North Carolina) for days. We
believe that this paper will assist the respective organizations, lawmakers,
and security professionals in understanding the scope of this issue in order to
prevent possible 911-DDoS attacks in the future.
</p>
    <p>Captured tweets and retweets: 8</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/smerity/status/774763166194475009"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1609.02087" target="_blank">Clearing the Skies: A deep network architecture for single-image rain
  removal</a></h4>
    <i>Xueyang Fu, Jiabin Huang, Xinghao Ding, Yinghao Liao, John Paisley</i>
    <p>  We introduce a deep network architecture called DerainNet for removing rain
streaks from an image. Based on the deep convolutional neural network (CNN), we
directly learn the mapping relationship between rainy and clean image detail
layers from data. Because we do not possess the ground truth corresponding to
real-world rainy images, we synthesize images with rain for training. To
effectively and efficiently train the network, different with common strategies
that roughly increase depth or breadth of network, we utilize some image
processing domain knowledge to modify the objective function. Specifically, we
train our DerainNet on the detail layer rather than the image domain. Better
results can be obtained under the same net architecture. Though DerainNet is
trained on synthetic data, we still find that the learned network is very
effective on real-world images for testing. Moreover, we augment the CNN
framework with image enhancement to significantly improve the visual results.
Compared with state-of-the- art single image de-rain methods, our method has
better rain removal and much faster computation time after network training.
</p>
    <p>Captured tweets and retweets: 12</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/alexjc/status/773791610375143424"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1609.01704" target="_blank">Hierarchical Multiscale Recurrent Neural Networks</a></h4>
    <i>Junyoung Chung, Sungjin Ahn, Yoshua Bengio</i>
    <p>  Learning both hierarchical and temporal representation has been among the
long-standing challenges of recurrent neural networks. Multiscale recurrent
neural networks have been considered as a promising approach to resolve this
issue, yet there has been a lack of empirical evidence showing that this type
of models can actually capture the temporal dependencies by discovering the
latent hierarchical structure of the sequence. In this paper, we propose a
novel multiscale approach, called the hierarchical multiscale recurrent neural
networks, which can capture the latent hierarchical structure in the sequence
by encoding the temporal dependencies with different timescales using a novel
update mechanism. We show some evidence that our proposed multiscale
architecture can discover underlying hierarchical structure in the sequences
without using explicit boundary information. We evaluate our proposed model on
character-level language modelling and handwriting sequence modelling.
</p>
    <p>Captured tweets and retweets: 53</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/nandodf/status/773422416571883524"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1609.00408" target="_blank">Defeating Image Obfuscation with Deep Learning</a></h4>
    <i>Richard McPherson, Reza Shokri, Vitaly Shmatikov</i>
    <p>  We demonstrate that modern image recognition methods based on artificial
neural networks can recover hidden information from images protected by various
forms of obfuscation. The obfuscation techniques considered in this paper are
mosaicing (also known as pixelation), blurring (as used by YouTube), and P3, a
recently proposed system for privacy-preserving photo sharing that encrypts the
significant JPEG coefficients to make images unrecognizable by humans. We
empirically show how to train artificial neural networks to successfully
identify faces and recognize objects and handwritten digits even if the images
are protected using any of the above obfuscation techniques.
</p>
    <p>Captured tweets and retweets: 78</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/alexjc/status/772721415317585920"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1609.00150" target="_blank">Reward Augmented Maximum Likelihood for Neural Structured Prediction</a></h4>
    <i>Mohammad Norouzi, Samy Bengio, Zhifeng Chen, Navdeep Jaitly, Mike Schuster, Yonghui Wu, Dale Schuurmans</i>
    <p>  A key problem in structured output prediction is direct optimization of the
task reward function that matters for test evaluation. This paper presents a
simple and computationally efficient approach to incorporate task reward into a
maximum likelihood framework. We establish a connection between the
log-likelihood and regularized expected reward objectives, showing that at a
zero temperature, they are approximately equivalent in the vicinity of the
optimal solution. We show that optimal regularized expected reward is achieved
when the conditional distribution of the outputs given the inputs is
proportional to their exponentiated (temperature adjusted) rewards. Based on
this observation, we optimize conditional log-probability of edited outputs
that are sampled proportionally to their scaled exponentiated reward. We apply
this framework to optimize edit distance in the output label space. Experiments
on speech recognition and machine translation for neural sequence to sequence
models show notable improvements over a maximum likelihood baseline by using
edit distance augmented maximum likelihood.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/j_gauthier/status/773364397192261633"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1609.00037" target="_blank">Good Enough Practices in Scientific Computing</a></h4>
    <i>Greg Wilson, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, Tracy K. Teal</i>
    <p>  We present a set of computing tools and techniques that every researcher can
and should adopt. These recommendations synthesize inspiration from our own
work, from the experiences of the thousands of people who have taken part in
Software Carpentry and Data Carpentry workshops over the past six years, and
from a variety of other guides. Unlike some other guides, our recommendations
are aimed specifically at people who are new to research computing.
</p>
    <p>Captured tweets and retweets: 23</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/deliprao/status/773221533846122496"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.08868" target="_blank">Demographic Dialectal Variation in Social Media: A Case Study of
  African-American English</a></h4>
    <i>Su Lin Blodgett, Lisa Green, Brendan O&#39;Connor</i>
    <p>  Though dialectal language is increasingly abundant on social media, few
resources exist for developing NLP tools to handle such language. We conduct a
case study of dialectal language in online conversational text by investigating
African-American English (AAE) on Twitter. We propose a distantly supervised
model to identify AAE-like language from demographics associated with
geo-located messages, and we verify that this language follows well-known AAE
linguistic phenomena. In addition, we analyze the quality of existing language
identification and dependency parsing tools on AAE-like text, demonstrating
that they perform poorly on such text compared to text associated with white
speakers. We also provide an ensemble classifier for language identification
which eliminates this disparity and release a new corpus of tweets containing
AAE-like language.
</p>
    <p>Captured tweets and retweets: 4</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/stanfordnlp/status/773535701841260544"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/brendan642/status/773933656050130949"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.08225" target="_blank">Why does deep and cheap learning work so well?</a></h4>
    <i>Henry W. Lin, Max Tegmark</i>
    <p>  We show how the success of deep learning depends not only on mathematics but
also on physics: although well-known mathematical theorems guarantee that
neural networks can approximate arbitrary functions well, the class of
functions of practical interest can be approximated through &#34;cheap learning&#34;
with exponentially fewer parameters than generic ones, because they have
simplifying properties tracing back to the laws of physics. The exceptional
simplicity of physics-based functions hinges on properties such as symmetry,
locality, compositionality and polynomial log-probability, and we explore how
these properties translate into exceptionally simple neural networks
approximating both natural phenomena such as images and abstract
representations thereof such as drawings. We further argue that when the
statistical process generating the data is of a certain hierarchical form
prevalent in physics and machine-learning, a deep neural network can be more
efficient than a shallow one. We formalize these claims using information
theory and discuss the relation to renormalization group procedures. Various
&#34;no-flattening theorems&#34; show when these efficient deep networks cannot be
accurately approximated by shallow ones without efficiency loss - even for
linear networks.
</p>
    <p>Captured tweets and retweets: 6</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/deliprao/status/770788372491210752"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/petewarden/status/771759924095377408"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.07905" target="_blank">Machine Comprehension Using Match-LSTM and Answer Pointer</a></h4>
    <i>Shuohang Wang, Jing Jiang</i>
    <p>  Machine comprehension of text is an important problem in natural language
processing. A recently released dataset, the Stanford Question Answering
Dataset (SQuAD), offers a large number of real questions and their answers
created by humans through crowdsourcing. SQuAD provides a challenging testbed
for evaluating machine comprehension algorithms, partly because compared with
previous datasets, in SQuAD the answers do not come from a small set of
candidate answers and they have variable lengths. We propose an end-to-end
neural architecture for the task. The architecture is based on match-LSTM, a
model we proposed previously for textual entailment, and Pointer Net, a
sequence-to-sequence model proposed by Vinyals et al.(2015) to constrain the
output tokens to be from the input sequences. We propose two ways of using
Pointer Net for our task. Our experiments show that both of our two models
substantially outperform the best results obtained by Rajpurkar et al.(2016)
using logistic regression and manually crafted features.
</p>
    <p>Captured tweets and retweets: 20</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/fchollet/status/770750317596119041"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/stanfordnlp/status/772445357489819648"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.07068" target="_blank">Title Generation for User Generated Videos</a></h4>
    <i>Kuo-Hao Zeng, Tseng-Hung Chen, Juan Carlos Niebles, Min Sun</i>
    <p>  A great video title describes the most salient event compactly and captures
the viewer&#39;s attention. In contrast, video captioning tends to generate
sentences that describe the video as a whole. Although generating a video title
automatically is a very useful task, it is much less addressed than video
captioning. We address video title generation for the first time by proposing
two methods that extend state-of-the-art video captioners to this new task.
First, we make video captioners highlight sensitive by priming them with a
highlight detector. Our framework allows for jointly training a model for title
generation and video highlight localization. Second, we induce high sentence
diversity in video captioners, so that the generated titles are also diverse
and catchy. This means that a large number of sentences might be required to
learn the sentence structure of titles. Hence, we propose a novel sentence
augmentation method to train a captioner with additional sentence-only examples
that come without corresponding videos. We collected a large-scale Video Titles
in the Wild (VTW) dataset of 18100 automatically crawled user-generated videos
and titles. On VTW, our methods consistently improve title prediction accuracy,
and achieve the best performance in both automatic and human evaluation.
Finally, our sentence augmentation method also outperforms the baselines on the
M-VAD dataset.
</p>
    <p>Captured tweets and retweets: 3</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/alexjc/status/769076592396083202"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.06197" target="_blank">CrowdNet: A Deep Convolutional Network for Dense Crowd Counting</a></h4>
    <i>Lokesh Boominathan, Srinivas S S Kruthiventi, R. Venkatesh Babu</i>
    <p>  Our work proposes a novel deep learning framework for estimating crowd
density from static images of highly dense crowds. We use a combination of deep
and shallow, fully convolutional networks to predict the density map for a
given crowd image. Such a combination is used for effectively capturing both
the high-level semantic information (face/body detectors) and the low-level
features (blob detectors), that are necessary for crowd counting under large
scale variations. As most crowd datasets have limited training samples (&lt;100
images) and deep learning based approaches require large amounts of training
data, we perform multi-scale data augmentation. Augmenting the training samples
in such a manner helps in guiding the CNN to learn scale invariant
representations. Our method is tested on the challenging UCF_CC_50 dataset, and
shown to outperform the state of the art methods.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/deep_hub/status/768163257488121856"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.05343" target="_blank">Decoupled Neural Interfaces using Synthetic Gradients</a></h4>
    <i>Max Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, Koray Kavukcuoglu</i>
    <p>  Training directed neural networks typically requires forward-propagating data
through a computation graph, followed by backpropagating error signal, to
produce weight updates. All layers, or more generally, modules, of the network
are therefore locked, in the sense that they must wait for the remainder of the
network to execute forwards and propagate error backwards before they can be
updated. In this work we break this constraint by decoupling modules by
introducing a model of the future computation of the network graph. These
models predict what the result of the modelled subgraph will produce using only
local information. In particular we focus on modelling error gradients: by
using the modelled synthetic gradient in place of true backpropagated error
gradients we decouple subgraphs, and can update them independently and
asynchronously i.e. we realise decoupled neural interfaces. We show results for
feed-forward models, where every layer is trained asynchronously, recurrent
neural networks (RNNs) where predicting one&#39;s future gradient extends the time
over which the RNN can effectively model, and also a hierarchical RNN system
with ticking at different timescales. Finally, we demonstrate that in addition
to predicting gradients, the same framework can be used to predict inputs,
resulting in models which are decoupled in both the forward and backwards pass
-- amounting to independent networks which co-learn such that they can be
composed into a single functioning corporation.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/petewarden/status/766477174660476928"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.05148" target="_blank">Full Resolution Image Compression with Recurrent Neural Networks</a></h4>
    <i>George Toderici, Damien Vincent, Nick Johnston, Sung Jin Hwang, David Minnen, Joel Shor, Michele Covell</i>
    <p>  This paper presents a set of full-resolution lossy image compression methods
based on neural networks. Each of the architectures we describe can provide
variable compression rates during deployment without requiring retraining of
the network: each network need only be trained once. All of our architectures
consist of a recurrent neural network (RNN)-based encoder and decoder, a
binarizer, and a neural network for entropy coding. We compare RNN types (LSTM,
associative LSTM) and introduce a new hybrid of GRU and ResNet. We also study
&#34;one-shot&#34; versus additive reconstruction architectures and introduce a new
scaled-additive framework. We compare to previous work, showing improvements of
4.3%-8.8% AUC (area under the rate-distortion curve), depending on the
perceptual metric used. As far as we know, this is the first neural network
architecture that is able to outperform JPEG at image compression across most
bitrates on the rate-distortion curve on the Kodak dataset images, with and
without the aid of entropy coding.
</p>
    <p>Captured tweets and retweets: 3</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/alexjc/status/766611397627219968"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/petewarden/status/768110248091537409"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.05081" target="_blank">Efficient Exploration for Dialog Policy Learning with Deep BBQ Networks
  \&amp; Replay Buffer Spiking</a></h4>
    <i>Zachary C. Lipton, Jianfeng Gao, Lihong Li, Xiujun Li, Faisal Ahmed, Li Deng</i>
    <p>  When rewards are sparse and efficient exploration essential, deep Q-learning
with $\epsilon$-greedy exploration tends to fail. This poses problems for
otherwise promising domains such as task-oriented dialog systems, where the
primary reward signal, indicating successful completion, typically occurs only
at the end of each episode but depends on the entire sequence of utterances. A
poor agent encounters such successful dialogs rarely, and a random agent may
never stumble upon a successful outcome in reasonable time. We present two
techniques that significantly improve the efficiency of exploration for deep
Q-learning agents in dialog systems. First, we demonstrate that exploration by
Thompson sampling, using Monte Carlo samples from a Bayes-by-Backprop neural
network, yields marked improvement over standard DQNs with Boltzmann or
$\epsilon$-greedy exploration. Second, we show that spiking the replay buffer
with a small number of successes, as are easy to harvest for dialog tasks, can
make Q-learning feasible when it might otherwise fail catastrophically.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/boredyannlecun/status/766908552552132608"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.04631" target="_blank">Neural versus Phrase-Based Machine Translation Quality: a Case Study</a></h4>
    <i>Luisa Bentivogli, Arianna Bisazza, Mauro Cettolo, Marcello Federico</i>
    <p>  Within the field of Statistical Machine Translation (SMT), the neural
approach (NMT) has recently emerged as the first technology able to challenge
the long-standing dominance of phrase-based approaches (PBMT). In particular,
at the IWSLT 2015 evaluation campaign, NMT outperformed well established
state-of-the-art PBMT systems on English-German, a language pair known to be
particularly hard because of morphology and syntactic differences. To
understand in what respects NMT provides better translation quality than PBMT,
we perform a detailed analysis of neural versus phrase-based SMT outputs,
leveraging high quality post-edits performed by professional translators on the
IWSLT data. For the first time, our analysis provides useful insights on what
linguistic phenomena are best modeled by neural models -- such as the
reordering of verbs -- while pointing out other aspects that remain to be
improved.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/stanfordnlp/status/770705158451793920"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.04428" target="_blank">TerpreT: A Probabilistic Programming Language for Program Induction</a></h4>
    <i>Alexander L. Gaunt, Marc Brockschmidt, Rishabh Singh, Nate Kushman, Pushmeet Kohli, Jonathan Taylor, Daniel Tarlow</i>
    <p>  We study machine learning formulations of inductive program synthesis; given
input-output examples, we try to synthesize source code that maps inputs to
corresponding outputs. Our aims are to develop new machine learning approaches
based on neural networks and graphical models, and to understand the
capabilities of machine learning techniques relative to traditional
alternatives, such as those based on constraint solving from the programming
languages community.
  Our key contribution is the proposal of TerpreT, a domain-specific language
for expressing program synthesis problems. TerpreT is similar to a
probabilistic programming language: a model is composed of a specification of a
program representation (declarations of random variables) and an interpreter
describing how programs map inputs to outputs (a model connecting unknowns to
observations). The inference task is to observe a set of input-output examples
and infer the underlying program. TerpreT has two main benefits. First, it
enables rapid exploration of a range of domains, program representations, and
interpreter models. Second, it separates the model specification from the
inference algorithm, allowing like-to-like comparisons between different
approaches to inference. From a single TerpreT specification we automatically
perform inference using four different back-ends. These are based on gradient
descent, linear program (LP) relaxations for graphical models, discrete
satisfiability solving, and the Sketch program synthesis system.
  We illustrate the value of TerpreT by developing several interpreter models
and performing an empirical comparison between alternative inference
algorithms. Our key empirical finding is that constraint solvers dominate the
gradient descent and LP-based formulations. We conclude with suggestions for
the machine learning community to make progress on program synthesis.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/numbercrunching/status/766027803338928129"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.04062" target="_blank">Stacked Approximated Regression Machine: A Simple Deep Learning Approach</a></h4>
    <i>Zhangyang Wang, Shiyu Chang, Qing Ling, Shuai Huang, Xia Hu, Honghui Shi, Thomas S. Huang</i>
    <p>  With the agreement of my coauthors, I Zhangyang Wang would like to withdraw
the manuscript “Stacked Approximated Regression Machine: A Simple Deep Learning
Approach”. Some experimental procedures were not included in the manuscript,
which makes a part of important claims not meaningful. In the relevant
research, I was solely responsible for carrying out the experiments; the other
coauthors joined in the discussions leading to the main algorithm.
  Please see the updated text for more details.
</p>
    <p>Captured tweets and retweets: 89</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/deliprao/status/774301347021586432"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/fchollet/status/774065138592690176"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/randal_olson/status/774276161270448132"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/deep_hub/status/771846438795882496"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/fchollet/status/771862837819867136"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/graycrawford/status/771844248929009664"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/karpathy/status/771822553903923202"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/tim_zaman/status/772029237855416320"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/dennybritz/status/774067398307553280"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.03983" target="_blank">SGDR: Stochastic Gradient Descent with Restarts</a></h4>
    <i>Ilya Loshchilov, Frank Hutter</i>
    <p>  Restart techniques are common in gradient-free optimization to deal with
multimodal functions. Partial restarts are also gaining popularity in
gradient-based optimization to improve the rate of convergence in accelerated
gradient schemes to deal with ill-conditioned functions. In this paper, we
propose a simple restart technique for stochastic gradient descent to improve
its anytime performance when training deep neural networks. We empirically
study its performance on CIFAR-10 and CIFAR-100 datasets where we demonstrate
new state-of-the-art results below 4\% and 19\%, respectively. Our source code
is available at https://github.com/loshchil/SGDR.
</p>
    <p>Captured tweets and retweets: 5</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/alexjc/status/766236305055514625"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.03819" target="_blank">DeepDiary: Automatic Caption Generation for Lifelogging Image Streams</a></h4>
    <i>Chenyou Fan, David J. Crandall</i>
    <p>  Lifelogging cameras capture everyday life from a first-person perspective,
but generate so much data that it is hard for users to browse and organize
their image collections effectively. In this paper, we propose to use automatic
image captioning algorithms to generate textual representations of these
collections. We develop and explore novel techniques based on deep learning to
generate captions for both individual images and image streams, using temporal
consistency constraints to create summaries that are both more compact and less
noisy. We evaluate our techniques with quantitative and qualitative results,
and apply captioning to an image retrieval application for finding potentially
private images. Our results suggest that our automatic captioning algorithms,
while imperfect, may work well enough to help users manage lifelogging photo
collections.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/alexjc/status/765118769849069568"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.03785" target="_blank">Compositional Distributional Cognition</a></h4>
    <i>Yaared Al-Mehairi, Bob Coecke, Martha Lewis</i>
    <p>  We accommodate the Integrated Connectionist/Symbolic Architecture (ICS) of
[32] within the categorical compositional semantics (CatCo) of [13], forming a
model of categorical compositional cognition (CatCog). This resolves intrinsic
problems with ICS such as the fact that representations inhabit an unbounded
space and that sentences with differing tree structures cannot be directly
compared. We do so in a way that makes the most of the grammatical structure
available, in contrast to strategies like circular convolution. Using the CatCo
model also allows us to make use of tools developed for CatCo such as the
representation of ambiguity and logical reasoning via density matrices,
structural meanings for words such as relative pronouns, and addressing over-
and under-extension, all of which are present in cognitive processes. Moreover
the CatCog framework is sufficiently flexible to allow for entirely different
representations of meaning, such as conceptual spaces. Interestingly, since the
CatCo model was largely inspired by categorical quantum mechanics, so is
CatCog.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/miles_brundage/status/764989658430341120"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.03665" target="_blank">Learning Structured Sparsity in Deep Neural Networks</a></h4>
    <i>Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, Hai Li</i>
    <p>  High demand for computation resources severely hinders deployment of
large-scale Deep Neural Networks (DNN) in resource constrained devices. In this
work, we propose a Structured Sparsity Learning (SSL) method to regularize the
structures (i.e., filters, channels, filter shapes, and layer depth) of DNNs.
SSL can: (1) learn a compact structure from a bigger DNN to reduce computation
cost; (2) obtain a hardware-friendly structured sparsity of DNN to efficiently
accelerate the DNNs evaluation. Experimental results show that SSL achieves on
average 5.1x and 3.1x speedups of convolutional layer computation of AlexNet
against CPU and GPU, respectively, with off-the-shelf libraries. These speedups
are about twice speedups of non-structured sparsity; (3) regularize the DNN
structure to improve classification accuracy. The results show that for
CIFAR-10, regularization on layer depth can reduce 20 layers of a Deep Residual
Network (ResNet) to 18 layers while improve the accuracy from 91.25% to 92.60%,
which is still slightly higher than that of original ResNet with 32 layers. For
AlexNet, structure regularization by SSL also reduces the error by around ~1%.
Open source code is in https://github.com/wenwei202/caffe/tree/scnn.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/miles_brundage/status/764986004457525248"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.03609" target="_blank">Clockwork Convnets for Video Semantic Segmentation</a></h4>
    <i>Evan Shelhamer, Kate Rakelly, Judy Hoffman, Trevor Darrell</i>
    <p>  Recent years have seen tremendous progress in still-image segmentation;
however the na\&#34;ive application of these state-of-the-art algorithms to every
video frame requires considerable computation and ignores the temporal
continuity inherent in video. We propose a video recognition framework that
relies on two key observations: 1) while pixels may change rapidly from frame
to frame, the semantic content of a scene evolves more slowly, and 2) execution
can be viewed as an aspect of architecture, yielding purpose-fit computation
schedules for networks. We define a novel family of &#34;clockwork&#34; convnets driven
by fixed or adaptive clock signals that schedule the processing of different
layers at different update rates according to their semantic stability. We
design a pipeline schedule to reduce latency for real-time recognition and a
fixed-rate schedule to reduce overall computation. Finally, we extend clockwork
scheduling to adaptive video processing by incorporating data-driven clocks
that can be tuned on unlabeled video. The accuracy and efficiency of clockwork
convnets are evaluated on the Youtube-Objects, NYUD, and Cityscapes video
datasets.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/miles_brundage/status/764986762011836416"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.02908" target="_blank">Residual Networks of Residual Networks: Multilevel Residual Networks</a></h4>
    <i>Ke Zhang, Miao Sun, Tony X. Han, Xingfang Yuan, Liru Guo, Tao Liu</i>
    <p>  Residual networks family with hundreds or even thousands of layers dominate
major image recognition tasks, but building a network by simply stacking
residual blocks inevitably limits its optimization ability. This paper proposes
a novel residual-network architecture, Residual networks of Residual networks
(RoR), to dig the optimization ability of residual networks. RoR substitutes
optimizing residual mapping of residual mapping for optimizing original
residual mapping, in particular, adding level-wise shortcut connections upon
original residual networks, to promote the learning capability of residual
networks. More importantly, RoR can be applied to various kinds of residual
networks (Pre-ResNets and WRN) and significantly boost their performance. Our
experiments demonstrate the effectiveness and versatility of RoR, where it
achieves the best performance in all residual-network-like structures. Our
RoR-3-WRN58-4 models achieve new state-of-the-art results on CIFAR-10,
CIFAR-100 and SVHN, with test errors 3.77%, 19.73% and 1.59% respectively.
These results outperform 1001-layer Pre-ResNets by 18.4% on CIFAR-10 and 13.1%
on CIFAR-100.
</p>
    <p>Captured tweets and retweets: 7</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/nandodf/status/763367917522067457"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/deep_hub/status/763423443245293568"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/ogrisel/status/763389214851346434"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/miles_brundage/status/763176263704084484"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.00367" target="_blank">Accelerating the Super-Resolution Convolutional Neural Network</a></h4>
    <i>Chao Dong, Chen Change Loy, Xiaoou Tang</i>
    <p>  As a successful deep model applied in image super-resolution (SR), the
Super-Resolution Convolutional Neural Network (SRCNN) has demonstrated superior
performance to the previous hand-crafted models either in speed and restoration
quality. However, the high computational cost still hinders it from practical
usage that demands real-time performance (24 fps). In this paper, we aim at
accelerating the current SRCNN, and propose a compact hourglass-shape CNN
structure for faster and better SR. We re-design the SRCNN structure mainly in
three aspects. First, we introduce a deconvolution layer at the end of the
network, then the mapping is learned directly from the original low-resolution
image (without interpolation) to the high-resolution one. Second, we
reformulate the mapping layer by shrinking the input feature dimension before
mapping and expanding back afterwards. Third, we adopt smaller filter sizes but
more mapping layers. The proposed model achieves a speed up of more than 40
times with even superior restoration quality. Further, we present the parameter
settings that can achieve real-time performance on a generic CPU while still
maintaining good performance. A corresponding transfer strategy is also
proposed for fast training and testing across different upscaling factors.
</p>
    <p>Captured tweets and retweets: 3</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/alexjc/status/760425562632577024"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.00318" target="_blank">A Neural Knowledge Language Model</a></h4>
    <i>Sungjin Ahn, Heeyoul Choi, Tanel Pärnamaa, Yoshua Bengio</i>
    <p>  Communicating knowledge is a primary purpose of language. However, current
language models have significant limitations in their ability to encode or
decode knowledge. This is mainly because they acquire knowledge based on
statistical co-occurrences, even if most of the knowledge words are rarely
observed named entities. In this paper, we propose a Neural Knowledge Language
Model (NKLM) which combines symbolic knowledge provided by knowledge graphs
with RNN language models. At each time step, the model predicts a fact on which
the observed word is supposed to be based. Then, a word is either generated
from the vocabulary or copied from the knowledge graph. We train and test the
model on a new dataset, WikiFacts. In experiments, we show that the NKLM
significantly improves the perplexity while generating a much smaller number of
unknown words. In addition, we demonstrate that the sampled descriptions
include named entities which were used to be the unknown words in RNN language
models.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/mat_kelcey/status/762391160467763200"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1607.07195" target="_blank">Higher-Order Factorization Machines</a></h4>
    <i>Mathieu Blondel, Akinori Fujino, Naonori Ueda, Masakazu Ishihata</i>
    <p>  Factorization machines (FMs) are a supervised learning approach that can use
second-order feature combinations even when the data is very high-dimensional.
Unfortunately, despite increasing interest in FMs, there exists to date no
efficient training algorithm for higher-order FMs (HOFMs). In this paper, we
present the first generic yet efficient algorithms for training arbitrary-order
HOFMs. We also present new variants of HOFMs with shared parameters, which
greatly reduce model size and prediction times while maintaining similar
accuracy. We demonstrate the proposed approaches on four different link
prediction tasks.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/mblondel_ml/status/764089614575427584"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1607.06450" target="_blank">Layer Normalization</a></h4>
    <i>Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton</i>
    <p>  Training state-of-the-art, deep neural networks is computationally expensive.
One way to reduce the training time is to normalize the activities of the
neurons. A recently introduced technique called batch normalization uses the
distribution of the summed input to a neuron over a mini-batch of training
cases to compute a mean and variance which are then used to normalize the
summed input to that neuron on each training case. This significantly reduces
the training time in feed-forward neural networks. However, the effect of batch
normalization is dependent on the mini-batch size and it is not obvious how to
apply it to recurrent neural networks. In this paper, we transpose batch
normalization into layer normalization by computing the mean and variance used
for normalization from all of the summed inputs to the neurons in a layer on a
single training case. Like batch normalization, we also give each neuron its
own adaptive bias and gain which are applied after the normalization but before
the non-linearity. Unlike batch normalization, layer normalization performs
exactly the same computation at training and test times. It is also
straightforward to apply to recurrent neural networks by computing the
normalization statistics separately at each time step. Layer normalization is
very effective at stabilizing the hidden state dynamics in recurrent networks.
Empirically, we show that layer normalization can substantially reduce the
training time compared with previously published techniques.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/richardsocher/status/756359202411520000"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1607.00133" target="_blank">Deep Learning with Differential Privacy</a></h4>
    <i>Martín Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, Li Zhang</i>
    <p>  Machine learning techniques based on neural networks are achieving remarkable
results in a wide variety of domains. Often, the training of models requires
large, representative datasets, which may be crowdsourced and contain sensitive
information. The models should not expose private information in these
datasets. Addressing this goal, we develop new algorithmic techniques for
learning and a refined analysis of privacy costs within the framework of
differential privacy. Our implementation and experiments demonstrate that we
can train deep neural networks with non-convex objectives, under a modest
privacy budget, and at a manageable cost in software complexity, training
efficiency, and model quality.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/jackclarksf/status/768750895681134592"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1606.09375" target="_blank">Convolutional Neural Networks on Graphs with Fast Localized Spectral
  Filtering</a></h4>
    <i>Michaël Defferrard, Xavier Bresson, Pierre Vandergheynst</i>
    <p>  Convolutional neural networks (CNNs) have greatly improved state-of-the-art
performances in a number of fields, notably computer vision and natural
language processing. In this work, we are interested in generalizing the
formulation of CNNs from low-dimensional regular Euclidean domains, where
images (2D), videos (3D) and audios (1D) are represented, to high-dimensional
irregular domains such as social networks or biological networks represented by
graphs. This paper introduces a formulation of CNNs on graphs in the context of
spectral graph theory. We borrow the fundamental tools from the emerging field
of signal processing on graphs, which provides the necessary mathematical
background and efficient numerical schemes to design localized graph filters
efficient to learn and evaluate. As a matter of fact, we introduce the first
technique that offers the same computational complexity than standard CNNs,
while being universal to any graph structure. Numerical experiments on MNIST
and 20NEWS demonstrate the ability of this novel deep learning system to learn
local, stationary, and compositional features on graphs, as long as the graph
is well-constructed.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/_onionesque/status/764536495743000576"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1606.06724" target="_blank">Tagger: Deep Unsupervised Perceptual Grouping</a></h4>
    <i>Klaus Greff, Antti Rasmus, Mathias Berglund, Tele Hotloo Hao, Jürgen Schmidhuber, Harri Valpola</i>
    <p>  We present a framework for efficient perceptual inference that explicitly
reasons about the segmentation of its inputs and features. Rather than being
trained for any specific segmentation, our framework learns the grouping
process in an unsupervised manner or alongside any supervised task. By
enriching the representations of a neural network, we enable it to group the
representations of different objects in an iterative manner. By allowing the
system to amortize the iterative inference of the groupings, we achieve very
fast convergence. In contrast to many other recently proposed methods for
addressing multi-object scenes, our system does not assume the inputs to be
images and can therefore directly handle other modalities. For multi-digit
classification of very cluttered images that require texture segmentation, our
method offers improved classification performance over convolutional networks
despite being fully connected. Furthermore, we observe that our system greatly
improves on the semi-supervised result of a baseline Ladder network on our
dataset, indicating that segmentation can also improve sample efficiency.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/nandodf/status/745965490305114112"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1606.06160" target="_blank">DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low
  Bitwidth Gradients</a></h4>
    <i>Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, Yuheng Zou</i>
    <p>  We propose DoReFa-Net, a method to train convolutional neural networks that
have low bitwidth weights and activations using low bitwidth parameter
gradients. In particular, during backward pass, parameter gradients are
stochastically quantized to low bitwidth numbers before being propagated to
convolutional layers. As convolutions during forward/backward passes can now
operate on low bitwidth weights and activations/gradients respectively,
DoReFa-Net can use bit convolution kernels to accelerate both training and
inference. Moreover, as bit convolutions can be efficiently implemented on CPU,
FPGA, ASIC and GPU, DoReFa-Net opens the way to accelerate training of low
bitwidth neural network on these hardware. Our experiments on SVHN and ImageNet
datasets prove that DoReFa-Net can achieve comparable prediction accuracy as
32-bit counterparts. For example, a DoReFa-Net derived from AlexNet that has
1-bit weights, 2-bit activations, can be trained from scratch using 6-bit
gradients to get 46.1\% top-1 accuracy on ImageNet validation set. The
DoReFa-Net AlexNet model is released publicly.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/culurciello/status/747506476793495553"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1606.05328" target="_blank">Conditional Image Generation with PixelCNN Decoders</a></h4>
    <i>Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, Koray Kavukcuoglu</i>
    <p>  This work explores conditional image generation with a new image density
model based on the PixelCNN architecture. The model can be conditioned on any
vector, including descriptive labels or tags, or latent embeddings created by
other networks. When conditioned on class labels from the ImageNet database,
the model is able to generate diverse, realistic scenes representing distinct
animals, objects, landscapes and structures. When conditioned on an embedding
produced by a convolutional network given a single image of an unseen face, it
generates a variety of new portraits of the same person with different facial
expressions, poses and lighting conditions. We also show that conditional
PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally,
the gated convolutional layers in the proposed model improve the log-likelihood
of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet,
with greatly reduced computational cost.
</p>
    <p>Captured tweets and retweets: 3</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/petewarden/status/744692488657575937"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/lemonodor/status/743687707960893441"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1606.04442" target="_blank">DeepMath - Deep Sequence Models for Premise Selection</a></h4>
    <i>Alex A. Alemi, Francois Chollet, Geoffrey Irving, Christian Szegedy, Josef Urban</i>
    <p>  We study the effectiveness of neural sequence models for premise selection in
automated theorem proving, one of the main bottlenecks in the formalization of
mathematics. We propose a two stage approach for this task that yields good
results for the premise selection task on the Mizar corpus while avoiding the
hand-engineered features of existing state-of-the-art models. To our knowledge,
this is the first time deep learning has been applied to theorem proving.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/fchollet/status/742875527086628865"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1606.04080" target="_blank">Matching Networks for One Shot Learning</a></h4>
    <i>Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, Daan Wierstra</i>
    <p>  Learning from a few examples remains a key challenge in machine learning.
Despite recent advances in important domains such as vision and language, the
standard supervised deep learning paradigm does not offer a satisfactory
solution for learning new concepts rapidly from little data. In this work, we
employ ideas from metric learning based on deep neural features and from recent
advances that augment neural networks with external memories. Our framework
learns a network that maps a small labelled support set and an unlabelled
example to its label, obviating the need for fine-tuning to adapt to new class
types. We then define one-shot learning problems on vision (using Omniglot,
ImageNet) and language tasks. Our algorithm improves one-shot accuracy on
ImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to
competing approaches. We also demonstrate the usefulness of the same model on
language modeling by introducing a one-shot task on the Penn Treebank.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/oriolvinyalsml/status/742557162493386752"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1606.03498" target="_blank">Improved Techniques for Training GANs</a></h4>
    <i>Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen</i>
    <p>  We present a variety of new architectural features and training procedures
that we apply to the generative adversarial networks (GANs) framework. We focus
on two applications of GANs: semi-supervised learning, and the generation of
images that humans find visually realistic. Unlike most work on generative
models, our primary goal is not to train a model that assigns high likelihood
to test data, nor do we require the model to be able to learn well without
using any labels. Using our new techniques, we achieve state-of-the-art results
in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated
images are of high quality as confirmed by a visual Turing test: our model
generates MNIST samples that humans cannot distinguish from real data, and
CIFAR-10 samples that yield a human error rate of 21.3%. We also present
ImageNet samples with unprecedented resolution and show that our methods enable
the model to learn recognizable features of ImageNet classes.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/mat_kelcey/status/742840044734418945"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1606.02447" target="_blank">Learning Language Games through Interaction</a></h4>
    <i>Sida I. Wang, Percy Liang, Christopher D. Manning</i>
    <p>  We introduce a new language learning setting relevant to building adaptive
natural language interfaces. It is inspired by Wittgenstein&#39;s language games: a
human wishes to accomplish some task (e.g., achieving a certain configuration
of blocks), but can only communicate with a computer, who performs the actual
actions (e.g., removing all red blocks). The computer initially knows nothing
about language and therefore must learn it from scratch through interaction,
while the human adapts to the computer&#39;s capabilities. We created a game in a
blocks world and collected interactions from 100 people playing it. First, we
analyze the humans&#39; strategies, showing that using compositionality and
avoiding synonyms correlates positively with task performance. Second, we
compare computer strategies, showing how to quickly learn a semantic parsing
model from scratch, and that modeling pragmatics further accelerates learning
for successful players.
</p>
    <p>Captured tweets and retweets: 3</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/grantdelozier/status/763313602241716224"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/albertstartup/status/773006519243726848"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1606.01933" target="_blank">A Decomposable Attention Model for Natural Language Inference</a></h4>
    <i>Ankur P. Parikh, Oscar Täckström, Dipanjan Das, Jakob Uszkoreit</i>
    <p>  We propose a simple neural architecture for natural language inference. Our
approach uses attention to decompose the problem into subproblems that can be
solved separately, thus making it trivially parallelizable. On the Stanford
Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results
with almost an order of magnitude fewer parameters than previous work and
without relying on any word-order information. Adding intra-sentence attention
that takes a minimum amount of order into account yields further improvements.
</p>
    <p>Captured tweets and retweets: 33</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/yoavgo/status/772062665627795457"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1606.01455" target="_blank">Multimodal Residual Learning for Visual QA</a></h4>
    <i>Jin-Hwa Kim, Sang-Woo Lee, Dong-Hyun Kwak, Min-Oh Heo, Jeonghee Kim, Jung-Woo Ha, Byoung-Tak Zhang</i>
    <p>  Deep neural networks continue to advance the state-of-the-art of image
recognition tasks with various methods. However, applications of these methods
to multimodality remain limited. We present Multimodal Residual Networks (MRN)
for the multimodal residual learning of visual question-answering, which
extends the idea of the deep residual learning. Unlike the deep residual
learning, MRN effectively learns the joint representation from vision and
language information. The main idea is to use element-wise multiplication for
the joint residual mappings exploiting the residual learning of the attentional
models in recent studies. Various alternative models introduced by
multimodality are explored based on our study. We achieve the state-of-the-art
results on the Visual QA dataset for both Open-Ended and Multiple-Choice tasks.
Moreover, we introduce a novel method to visualize the attention effect of the
joint representations for each learning block using back-propagation algorithm,
even though the visual features are collapsed without spatial information.
</p>
    <p>Captured tweets and retweets: 3</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/jnhwkim/status/771674699155648512"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/jnhwkim/status/771153382140579840"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/jnhwkim/status/763891103258910720"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1606.01305" target="_blank">Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations</a></h4>
    <i>David Krueger, Tegan Maharaj, János Kramár, Mohammad Pezeshki, Nicolas Ballas, Nan Rosemary Ke, Anirudh Goyal, Yoshua Bengio, Hugo Larochelle, Aaron Courville, Chris Pal</i>
    <p>  We propose zoneout, a novel method for regularizing RNNs. At each timestep,
zoneout stochastically forces some hidden units to maintain their previous
values. Like dropout, zoneout uses random noise to train a pseudo-ensemble,
improving generalization. But by preserving instead of dropping hidden units,
gradient information and state information are more readily propagated through
time, as in feedforward stochastic depth networks. We perform an empirical
investigation of various RNN regularizers, and find encouraging results:
zoneout gives significant performance improvements across tasks, yielding
state-of-the-art results in character-level language modeling on the Penn
Treebank dataset and competitive results on word-level Penn Treebank and
permuted sequential MNIST classification tasks.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/karpathy/status/740045174634516480"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/smerity/status/771597610863857664"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1606.01167" target="_blank">How Deep is the Feature Analysis underlying Rapid Visual Categorization?</a></h4>
    <i>Sven Eberhardt, Jonah Cader, Thomas Serre</i>
    <p>  Rapid categorization paradigms have a long history in experimental
psychology: Characterized by short presentation times and speedy behavioral
responses, these tasks highlight the efficiency with which our visual system
processes natural object categories. Previous studies have shown that
feed-forward hierarchical models of the visual cortex provide a good fit to
human visual decisions. At the same time, recent work in computer vision has
demonstrated significant gains in object recognition accuracy with increasingly
deep hierarchical architectures. But it is unclear how well these models
account for human visual decisions and what they may reveal about the
underlying brain processes.
  We have conducted a large-scale psychophysics study to assess the correlation
between computational models and human participants on a rapid animal vs.
non-animal categorization task. We considered visual representations of varying
complexity by analyzing the output of different stages of processing in three
state-of-the-art deep networks. We found that recognition accuracy increases
with higher stages of visual processing (higher level stages indeed
outperforming human participants on the same task) but that human decisions
agree best with predictions from intermediate stages.
  Overall, these results suggest that human participants may rely on visual
features of intermediate complexity and that the complexity of visual
representations afforded by modern deep network models may exceed those used by
human participants during rapid categorization.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/karpathy/status/739693625945133057"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1605.09304" target="_blank">Synthesizing the preferred inputs for neurons in neural networks via
  deep generator networks</a></h4>
    <i>Anh Nguyen, Alexey Dosovitskiy, Jason Yosinski, Thomas Brox, Jeff Clune</i>
    <p>  Deep neural networks (DNNs) have demonstrated state-of-the-art results on
many pattern recognition tasks, especially vision classification problems.
Understanding the inner workings of such computational brains is both
fascinating basic science that is interesting in its own right - similar to why
we study the human brain - and will enable researchers to further improve DNNs.
One path to understanding how a neural network functions internally is to study
what each of its neurons has learned to detect. One such method is called
activation maximization (AM), which synthesizes an input (e.g. an image) that
highly activates a neuron. Here we dramatically improve the qualitative state
of the art of activation maximization by harnessing a powerful, learned prior:
a deep generator network (DGN). The algorithm (1) generates qualitatively
state-of-the-art synthetic images that look almost real, (2) reveals the
features learned by each neuron in an interpretable way, (3) generalizes well
to new datasets and somewhat well to different network architectures without
requiring the prior to be relearned, and (4) can be considered as a
high-quality generative method (in this case, by generating novel, creative,
interesting, recognizable images).
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/petewarden/status/738065771880800258"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1605.08695" target="_blank">TensorFlow: A system for large-scale machine learning</a></h4>
    <i>Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, Xiaoqiang Zheng</i>
    <p>  TensorFlow is a machine learning system that operates at large scale and in
heterogeneous environments. TensorFlow uses dataflow graphs to represent
computation, shared state, and the operations that mutate that state. It maps
the nodes of a dataflow graph across many machines in a cluster, and within a
machine across multiple computational devices, including multicore CPUs,
general-purpose GPUs, and custom designed ASICs known as Tensor Processing
Units (TPUs). This architecture gives flexibility to the application developer:
whereas in previous &#34;parameter server&#34; designs the management of shared state
is built into the system, TensorFlow enables developers to experiment with
novel optimizations and training algorithms. TensorFlow supports a variety of
applications, with particularly strong support for training and inference on
deep neural networks. Several Google services use TensorFlow in production, we
have released it as an open-source project, and it has become widely used for
machine learning research. In this paper, we describe the TensorFlow dataflow
model in contrast to existing systems, and demonstrate the compelling
performance that TensorFlow achieves for several real-world applications.
</p>
    <p>Captured tweets and retweets: 4</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/mrry/status/759525483541245952"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/mrry/status/755554043347341312"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/mrry/status/737645093398532098"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1605.08104" target="_blank">Deep Predictive Coding Networks for Video Prediction and Unsupervised
  Learning</a></h4>
    <i>William Lotter, Gabriel Kreiman, David Cox</i>
    <p>  While great strides have been made in using deep learning algorithms to solve
supervised learning tasks, the problem of unsupervised learning - leveraging
unlabeled examples to learn about the structure of a domain - remains a
difficult unsolved challenge. Here, we explore prediction of future frames in a
video sequence as an unsupervised learning rule for learning about the
structure of the visual world. We describe a predictive neural network
(&#34;PredNet&#34;) architecture that is inspired by the concept of &#34;predictive coding&#34;
from the neuroscience literature. These networks learn to predict future frames
in a video sequence, with each layer in the network making local predictions
and only forwarding deviations from those predictions to subsequent network
layers. We show that these networks are able to robustly learn to predict the
movement of synthetic (rendered) objects, and that in doing so, the networks
learn internal representations that are useful for decoding latent object
parameters (e.g. pose) that support object recognition with fewer training
views. We also show that these networks can scale to complex natural image
streams (car-mounted camera videos), capturing key aspects of both egocentric
movement and the movement of objects in the visual scene, and generalizing
across video datasets. These results suggest that prediction represents a
powerful framework for unsupervised learning, allowing for implicit learning of
object and scene structure.
</p>
    <p>Captured tweets and retweets: 17</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/deliprao/status/771421498573725696"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1605.07648" target="_blank">FractalNet: Ultra-Deep Neural Networks without Residuals</a></h4>
    <i>Gustav Larsson, Michael Maire, Gregory Shakhnarovich</i>
    <p>  We introduce a design strategy for neural network macro-architecture based on
self-similarity. Repeated application of a single expansion rule generates an
extremely deep network whose structural layout is precisely a truncated
fractal. Such a network contains interacting subpaths of different lengths, but
does not include any pass-through connections: every internal signal is
transformed by a filter and nonlinearity before being seen by subsequent
layers. This property stands in stark contrast to the current approach of
explicitly structuring very deep networks so that training is a residual
learning problem. Our experiments demonstrate that residual representation is
not fundamental to the success of extremely deep convolutional neural networks.
A fractal design achieves an error rate of 22.85% on CIFAR-100, matching the
state-of-the-art held by residual networks.
  Fractal networks exhibit intriguing properties beyond their high performance.
They can be regarded as a computationally efficient implicit union of
subnetworks of every depth. We explore consequences for training, touching upon
connection with student-teacher behavior, and, most importantly, demonstrating
the ability to extract high-performance fixed-depth subnetworks. To facilitate
this latter task, we develop drop-path, a natural extension of dropout, to
regularize co-adaptation of subpaths in fractal architectures. With such
regularization, fractal networks exhibit an anytime property: shallow
subnetworks provide a quick answer, while deeper subnetworks, with higher
latency, provide a more accurate answer.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/karpathy/status/737055043153580032"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1605.06457" target="_blank">Virtual Worlds as Proxy for Multi-Object Tracking Analysis</a></h4>
    <i>Adrien Gaidon, Qiao Wang, Yohann Cabon, Eleonora Vig</i>
    <p>  Modern computer vision algorithms typically require expensive data
acquisition and accurate manual labeling. In this work, we instead leverage the
recent progress in computer graphics to generate fully labeled, dynamic, and
photo-realistic proxy virtual worlds. We propose an efficient real-to-virtual
world cloning method, and validate our approach by building and publicly
releasing a new video dataset, called Virtual KITTI (see
http://www.xrce.xerox.com/Research-Development/Computer-Vision/Proxy-Virtual-Worlds),
automatically labeled with accurate ground truth for object detection,
tracking, scene and instance segmentation, depth, and optical flow. We provide
quantitative experimental evidence suggesting that (i) modern deep learning
algorithms pre-trained on real data behave similarly in real and virtual
worlds, and (ii) pre-training on virtual data improves performance. As the gap
between real and virtual worlds is small, virtual worlds enable measuring the
impact of various weather and imaging conditions on recognition performance,
all other things being equal. We show these factors may affect drastically
otherwise high-performing deep models for tracking.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/karpathy/status/755344585539346432"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1605.05396" target="_blank">Generative Adversarial Text to Image Synthesis</a></h4>
    <i>Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, Honglak Lee</i>
    <p>  Automatic synthesis of realistic images from text would be interesting and
useful, but current AI systems are still far from this goal. However, in recent
years generic and powerful recurrent neural network architectures have been
developed to learn discriminative text feature representations. Meanwhile, deep
convolutional generative adversarial networks (GANs) have begun to generate
highly compelling images of specific categories, such as faces, album covers,
and room interiors. In this work, we develop a novel deep architecture and GAN
formulation to effectively bridge these advances in text and image model- ing,
translating visual concepts from characters to pixels. We demonstrate the
capability of our model to generate plausible images of birds and flowers from
detailed text descriptions.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/ethancaballero/status/772298100132057090"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1605.05273" target="_blank">Learning Convolutional Neural Networks for Graphs</a></h4>
    <i>Mathias Niepert, Mohamed Ahmed, Konstantin Kutzkov</i>
    <p>  Numerous important problems can be framed as learning from graph data. We
propose a framework for learning convolutional neural networks for arbitrary
graphs. These graphs may be undirected, directed, and with both discrete and
continuous node and edge attributes. Analogous to image-based convolutional
networks that operate on locally connected regions of the input, we present a
general approach to extracting locally connected regions from graphs. Using
established benchmark data sets, we demonstrate that the learned feature
representations are competitive with state of the art graph kernels and that
their computation is highly efficient.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/_onionesque/status/732747325928509440"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1605.04462" target="_blank">Large-scale Analysis of Counseling Conversations: An Application of
  Natural Language Processing to Mental Health</a></h4>
    <i>Tim Althoff, Kevin Clark, Jure Leskovec</i>
    <p>  Mental illness is one of the most pressing public health issues of our time.
While counseling and psychotherapy can be effective treatments, our knowledge
about how to conduct successful counseling conversations has been limited due
to lack of large-scale data with labeled outcomes of the conversations. In this
paper, we present a large-scale, quantitative study on the discourse of
text-message-based counseling conversations. We develop a set of novel
computational discourse analysis methods to measure how various linguistic
aspects of conversations are correlated with conversation outcomes. Applying
techniques such as sequence-based conversation models, language model
comparisons, message clustering, and psycholinguistics-inspired word frequency
analyses, we discover actionable conversation strategies that are associated
with better conversation outcomes.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/miles_brundage/status/732433579918163968"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1604.04004" target="_blank">Understanding How Image Quality Affects Deep Neural Networks</a></h4>
    <i>Samuel Dodge, Lina Karam</i>
    <p>  Image quality is an important practical challenge that is often overlooked in
the design of machine vision systems. Commonly, machine vision systems are
trained and tested on high quality image datasets, yet in practical
applications the input images can not be assumed to be of high quality.
Recently, deep neural networks have obtained state-of-the-art performance on
many machine vision tasks. In this paper we provide an evaluation of 4
state-of-the-art deep neural network models for image classification under
quality distortions. We consider five types of quality distortions: blur,
noise, contrast, JPEG, and JPEG2000 compression. We show that the existing
networks are susceptible to these quality distortions, particularly to blur and
noise. These results enable future work in developing deep neural networks that
are more invariant to quality distortions.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/kleinsound/status/723953721604890625"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1604.01692" target="_blank">An Ensemble Method to Produce High-Quality Word Embeddings</a></h4>
    <i>Robert Speer, Joshua Chin</i>
    <p>  A currently successful approach to computational semantics is to represent
words as embeddings in a machine-learned vector space. We present an ensemble
method that combines embeddings produced by GloVe (Pennington et al., 2014) and
word2vec (Mikolov et al., 2013) with structured knowledge from the semantic
networks ConceptNet (Speer and Havasi, 2012) and PPDB (Ganitkevitch et al.,
2013), merging their information into a common representation with a large,
multilingual vocabulary. The embeddings it produces achieve state-of-the-art
performance on many word-similarity evaluations. Its score of $\rho = .596$ on
an evaluation of rare words (Luong et al., 2013) is 16% higher than the
previous best known system.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/r_speer/status/771798224168480768"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1603.08233" target="_blank">Evolution of active categorical image classification via saccadic eye
  movement</a></h4>
    <i>Randal S. Olson, Jason H. Moore, Christoph Adami</i>
    <p>  Pattern recognition and classification is a central concern for modern
information processing systems. In particular, one key challenge to image and
video classification has been that the computational cost of image processing
scales linearly with the number of pixels in the image or video. Here we
present an intelligent machine (the &#34;active categorical classifier,&#34; or ACC)
that is inspired by the saccadic movements of the eye, and is capable of
classifying images by selectively scanning only a portion of the image. We
harness evolutionary computation to optimize the ACC on the MNIST hand-written
digit classification task, and provide a proof-of-concept that the ACC works on
noisy multi-class data. We further analyze the ACC and demonstrate its ability
to classify images after viewing only a fraction of the pixels, and provide
insight on future research paths to further improve upon the ACC presented
here.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/doctorjosh/status/764643870491770885"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1603.08155" target="_blank">Perceptual Losses for Real-Time Style Transfer and Super-Resolution</a></h4>
    <i>Justin Johnson, Alexandre Alahi, Li Fei-Fei</i>
    <p>  We consider image transformation problems, where an input image is
transformed into an output image. Recent methods for such problems typically
train feed-forward convolutional neural networks using a \emph{per-pixel} loss
between the output and ground-truth images. Parallel work has shown that
high-quality images can be generated by defining and optimizing
\emph{perceptual} loss functions based on high-level features extracted from
pretrained networks. We combine the benefits of both approaches, and propose
the use of perceptual loss functions for training feed-forward networks for
image transformation tasks. We show results on image style transfer, where a
feed-forward network is trained to solve the optimization problem proposed by
Gatys et al in real-time. Compared to the optimization-based method, our
network gives similar qualitative results but is three orders of magnitude
faster. We also experiment with single-image super-resolution, where replacing
a per-pixel loss with a perceptual loss gives visually pleasing results.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/alexjc/status/772530088386715648"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1603.05118" target="_blank">Recurrent Dropout without Memory Loss</a></h4>
    <i>Stanislau Semeniuta, Aliaksei Severyn, Erhardt Barth</i>
    <p>  This paper presents a novel approach to recurrent neural network (RNN)
regularization. Differently from the widely adopted dropout method, which is
applied to \textit{forward} connections of feed-forward architectures or RNNs,
we propose to drop neurons directly in \textit{recurrent} connections in a way
that does not cause loss of long-term memory. Our approach is as easy to
implement and apply as the regular feed-forward dropout and we demonstrate its
effectiveness for Long Short-Term Memory network, the most popular type of RNN
cells. Our experiments on NLP benchmarks show consistent improvements even when
combined with conventional feed-forward dropout.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/ozan__caglayan/status/771594297523089408"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1603.01417" target="_blank">Dynamic Memory Networks for Visual and Textual Question Answering</a></h4>
    <i>Caiming Xiong, Stephen Merity, Richard Socher</i>
    <p>  Neural network architectures with memory and attention mechanisms exhibit
certain reasoning capabilities required for question answering. One such
architecture, the dynamic memory network (DMN), obtained high accuracy on a
variety of language tasks. However, it was not shown whether the architecture
achieves strong results for question answering when supporting facts are not
marked during training or whether it could be applied to other modalities such
as images. Based on an analysis of the DMN, we propose several improvements to
its memory and input modules. Together with these changes we introduce a novel
input module for images in order to be able to answer visual questions. Our new
DMN+ model improves the state of the art on both the Visual Question Answering
dataset and the \babi-10k text question-answering dataset without supporting
fact supervision.
</p>
    <p>Captured tweets and retweets: 7</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/richardsocher/status/724369376925749248"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/stephenpiment/status/706967564732096513"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/smerity/status/706693233972129792"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/metamindio/status/706681498531864576"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1602.02410" target="_blank">Exploring the Limits of Language Modeling</a></h4>
    <i>Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, Yonghui Wu</i>
    <p>  In this work we explore recent advances in Recurrent Neural Networks for
large scale Language Modeling, a task central to language understanding. We
extend current models to deal with two key challenges present in this task:
corpora and vocabulary sizes, and complex, long term structure of language. We
perform an exhaustive study on techniques such as character Convolutional
Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark.
Our best single model significantly improves state-of-the-art perplexity from
51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20),
while an ensemble of models sets a new record by improving perplexity from 41.0
down to 23.7. We also release these models for the NLP and ML community to
study and improve upon.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/oriolvinyalsml/status/697078702006472704"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1601.04589" target="_blank">Combining Markov Random Fields and Convolutional Neural Networks for
  Image Synthesis</a></h4>
    <i>Chuan Li, Michael Wand</i>
    <p>  This paper studies a combination of generative Markov random field (MRF)
models and discriminatively trained deep convolutional neural networks (dCNNs)
for synthesizing 2D images. The generative MRF acts on higher-levels of a dCNN
feature pyramid, controling the image layout at an abstract level. We apply the
method to both photographic and non-photo-realistic (artwork) synthesis tasks.
The MRF regularizer prevents over-excitation artifacts and reduces implausible
feature mixtures common to previous dCNN inversion approaches, permitting
synthezing photographic content with increased visual plausibility. Unlike
standard MRF-based texture synthesis, the combined system can both match and
adapt local features with considerable variability, yielding results far out of
reach of classic generative MRF methods.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/amirsaffari/status/771626411094183936"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1512.05287" target="_blank">A Theoretically Grounded Application of Dropout in Recurrent Neural
  Networks</a></h4>
    <i>Yarin Gal</i>
    <p>  Recurrent neural networks (RNNs) stand at the forefront of many recent
developments in deep learning. Yet a major difficulty with these models is
their tendency to overfit, with dropout shown to fail when applied to recurrent
layers. Recent results at the intersection of Bayesian modelling and deep
learning offer a Bayesian interpretation of common deep learning techniques
such as dropout. This grounding of dropout in approximate Bayesian inference
suggests an extension of the theoretical results, offering insights into the
use of dropout with RNN models. We apply this new variational inference based
dropout technique in LSTM and GRU models, assessing it on language modelling
and sentiment analysis tasks. The new approach outperforms existing techniques,
and to the best of our knowledge improves on the single model state-of-the-art
in language modelling with the Penn Treebank (73.4 test perplexity). This
extends our arsenal of variational tools in deep learning.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/smerity/status/698649435044057088"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1511.07122" target="_blank">Multi-Scale Context Aggregation by Dilated Convolutions</a></h4>
    <i>Fisher Yu, Vladlen Koltun</i>
    <p>  State-of-the-art models for semantic segmentation are based on adaptations of
convolutional networks that had originally been designed for image
classification. However, dense prediction and image classification are
structurally different. In this work, we develop a new convolutional network
module that is specifically designed for dense prediction. The presented module
uses dilated convolutions to systematically aggregate multi-scale contextual
information without losing resolution. The architecture is based on the fact
that dilated convolutions support exponential expansion of the receptive field
without loss of resolution or coverage. We show that the presented context
module increases the accuracy of state-of-the-art semantic segmentation
systems. In addition, we examine the adaptation of image classification
networks to dense prediction and show that simplifying the adapted network can
increase accuracy.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/karpathy/status/729082742328033280"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1511.05756" target="_blank">Image Question Answering using Convolutional Neural Network with Dynamic
  Parameter Prediction</a></h4>
    <i>Hyeonwoo Noh, Paul Hongsuck Seo, Bohyung Han</i>
    <p>  We tackle image question answering (ImageQA) problem by learning a
convolutional neural network (CNN) with a dynamic parameter layer whose weights
are determined adaptively based on questions. For the adaptive parameter
prediction, we employ a separate parameter prediction network, which consists
of gated recurrent unit (GRU) taking a question as its input and a
fully-connected layer generating a set of candidate weights as its output.
However, it is challenging to construct a parameter prediction network for a
large number of parameters in the fully-connected dynamic parameter layer of
the CNN. We reduce the complexity of this problem by incorporating a hashing
technique, where the candidate weights given by the parameter prediction
network are selected using a predefined hash function to determine individual
weights in the dynamic parameter layer. The proposed network---joint network
with the CNN for ImageQA and the parameter prediction network---is trained
end-to-end through back-propagation, where its weights are initialized using a
pre-trained CNN and GRU. The proposed algorithm illustrates the
state-of-the-art performance on all available public ImageQA benchmarks.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/jnhwkim/status/763891103258910720"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1510.03055" target="_blank">A Diversity-Promoting Objective Function for Neural Conversation Models</a></h4>
    <i>Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, Bill Dolan</i>
    <p>  Sequence-to-sequence neural network models for generation of conversational
responses tend to generate safe, commonplace responses (e.g., &#34;I don&#39;t know&#34;)
regardless of the input. We suggest that the traditional objective function,
i.e., the likelihood of output (response) given input (message) is unsuited to
response generation tasks. Instead we propose using Maximum Mutual Information
(MMI) as the objective function in neural models. Experimental results
demonstrate that the proposed MMI models produce more diverse, interesting, and
appropriate responses, yielding substantive gains in BLEU scores on two
conversational datasets and in human evaluations.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/stanfordnlp/status/707588046745509888"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1506.08909" target="_blank">The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured
  Multi-Turn Dialogue Systems</a></h4>
    <i>Ryan Lowe, Nissan Pow, Iulian Serban, Joelle Pineau</i>
    <p>  This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost
1 million multi-turn dialogues, with a total of over 7 million utterances and
100 million words. This provides a unique resource for research into building
dialogue managers based on neural language models that can make use of large
amounts of unlabeled data. The dataset has both the multi-turn property of
conversations in the Dialog State Tracking Challenge datasets, and the
unstructured nature of interactions from microblog services such as Twitter. We
also describe two neural learning architectures suitable for analyzing this
dataset, and provide benchmark performance on the task of selecting the best
next response.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/nandodf/status/745972271467081728"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1506.07365" target="_blank">Embed to Control: A Locally Linear Latent Dynamics Model for Control
  from Raw Images</a></h4>
    <i>Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, Martin Riedmiller</i>
    <p>  We introduce Embed to Control (E2C), a method for model learning and control
of non-linear dynamical systems from raw pixel images. E2C consists of a deep
generative model, belonging to the family of variational autoencoders, that
learns to generate image trajectories from a latent space in which the dynamics
is constrained to be locally linear. Our model is derived directly from an
optimal control formulation in latent space, supports long-term prediction of
image sequences and exhibits strong performance on a variety of complex control
problems.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/mat_kelcey/status/763462515732865024"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1506.07285" target="_blank">Ask Me Anything: Dynamic Memory Networks for Natural Language Processing</a></h4>
    <i>Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, Richard Socher</i>
    <p>  Most tasks in natural language processing can be cast into question answering
(QA) problems over language input. We introduce the dynamic memory network
(DMN), a neural network architecture which processes input sequences and
questions, forms episodic memories, and generates relevant answers. Questions
trigger an iterative attention process which allows the model to condition its
attention on the inputs and the result of previous iterations. These results
are then reasoned over in a hierarchical recurrent sequence model to generate
answers. The DMN can be trained end-to-end and obtains state-of-the-art results
on several types of tasks and datasets: question answering (Facebook&#39;s bAbI
dataset), text classification for sentiment analysis (Stanford Sentiment
Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The
training for these different tasks relies exclusively on trained word vector
representations and input-question-answer triplets.
</p>
    <p>Captured tweets and retweets: 9</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/richardsocher/status/743214026264444928"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/richardsocher/status/724369376925749248"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/stephenpiment/status/702735139793539072"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/richardsocher/status/646739800012271616"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/richardsocher/status/646738608087367681"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/richardsocher/status/618565826082271232"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/stephenpiment/status/614433331955003393"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1506.01070" target="_blank">Do Multi-Sense Embeddings Improve Natural Language Understanding?</a></h4>
    <i>Jiwei Li, Dan Jurafsky</i>
    <p>  Learning a distinct representation for each sense of an ambiguous word could
lead to more powerful and fine-grained models of vector-space representations.
Yet while `multi-sense&#39; methods have been proposed and tested on artificial
word-similarity tasks, we don&#39;t know if they improve real natural language
understanding tasks. In this paper we introduce a multi-sense embedding model
based on Chinese Restaurant Processes that achieves state of the art
performance on matching human word similarity judgments, and propose a
pipelined architecture for incorporating multi-sense embeddings into language
understanding.
  We then test the performance of our model on part-of-speech tagging, named
entity recognition, sentiment analysis, semantic relation identification and
semantic relatedness, controlling for embedding dimensionality. We find that
multi-sense embeddings do improve performance on some tasks (part-of-speech
tagging, semantic relation identification, semantic relatedness) but not on
others (named entity recognition, various forms of sentiment analysis). We
discuss how these differences may be caused by the different role of word sense
information in each of the tasks. The results highlight the importance of
testing embedding models in real applications.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/david__jurgens/status/773303283247046656"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1506.01066" target="_blank">Visualizing and Understanding Neural Models in NLP</a></h4>
    <i>Jiwei Li, Xinlei Chen, Eduard Hovy, Dan Jurafsky</i>
    <p>  While neural networks have been successfully applied to many NLP tasks the
resulting vector-based models are very difficult to interpret. For example it&#39;s
not clear how they achieve {\em compositionality}, building sentence meaning
from the meanings of words and phrases. In this paper we describe four
strategies for visualizing compositionality in neural models for NLP, inspired
by similar work in computer vision. We first plot unit values to visualize
compositionality of negation, intensification, and concessive clauses, allow us
to see well-known markedness asymmetries in negation. We then introduce three
simple and straightforward methods for visualizing a unit&#39;s {\em salience}, the
amount it contributes to the final composed meaning: (1) gradient
back-propagation, (2) the variance of a token from the average word node, (3)
LSTM-style gates that measure information flow. We test our methods on
sentiment using simple recurrent nets and LSTMs. Our general-purpose methods
may have wide applications for understanding compositionality and other
semantic properties of deep networks , and also shed light on why LSTMs
outperform simple recurrent nets,
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/stanfordnlp/status/707588046745509888"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1505.01504" target="_blank">A Fixed-Size Encoding Method for Variable-Length Sequences with its
  Application to Neural Network Language Models</a></h4>
    <i>Shiliang Zhang, Hui Jiang, Mingbin Xu, Junfeng Hou, Lirong Dai</i>
    <p>  In this paper, we propose the new fixed-size ordinally-forgetting encoding
(FOFE) method, which can almost uniquely encode any variable-length sequence of
words into a fixed-size representation. FOFE can model the word order in a
sequence using a simple ordinally-forgetting mechanism according to the
positions of words. In this work, we have applied FOFE to feedforward neural
network language models (FNN-LMs). Experimental results have shown that without
using any recurrent feedbacks, FOFE based FNN-LMs can significantly outperform
not only the standard fixed-input FNN-LMs but also the popular RNN-LMs.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/richardsocher/status/597632520872595456"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1503.04069" target="_blank">LSTM: A Search Space Odyssey</a></h4>
    <i>Klaus Greff, Rupesh Kumar Srivastava, Jan Koutník, Bas R. Steunebrink, Jürgen Schmidhuber</i>
    <p>  Several variants of the Long Short-Term Memory (LSTM) architecture for
recurrent neural networks have been proposed since its inception in 1995. In
recent years, these networks have become the state-of-the-art models for a
variety of machine learning problems. This has led to a renewed interest in
understanding the role and utility of various computational components of
typical LSTM variants. In this paper, we present the first large-scale analysis
of eight LSTM variants on three representative tasks: speech recognition,
handwriting recognition, and polyphonic music modeling. The hyperparameters of
all LSTM variants for each task were optimized separately using random search
and their importance was assessed using the powerful fANOVA framework. In
total, we summarize the results of 5400 experimental runs (about 15 years of
CPU time), which makes our study the largest of its kind on LSTM networks. Our
results show that none of the variants can improve upon the standard LSTM
architecture significantly, and demonstrate the forget gate and the output
activation function to be its most critical components. We further observe that
the studied hyperparameters are virtually independent and derive guidelines for
their efficient adjustment.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/andrpem/status/769728735411003392"></a></blockquote></center>
    
  </div>
  <hr />
  

  </div>
</body>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
</html>