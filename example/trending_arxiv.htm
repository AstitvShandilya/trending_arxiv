<!doctype html>
<html lang="en">
<head>
  <title>Trending arXiv</title>
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
  <meta name="viewport" content="width=device-width, initial-scale=1">
</head>
<body>
  <div class="container">
  <h1>
    <a href="/refresh" title="Refresh"><span class="glyphicon glyphicon-refresh" aria-hidden="true"></span></a>
    Trending arXiv
  </h1>
  <ul class="nav nav-pills">
    <li role="presentation"><a href="/"><span aria-hidden="true" class="glyphicon glyphicon-file"></span> Papers</a></li>
    <li role="presentation"><a href="/tweets"><span aria-hidden="true" class="glyphicon glyphicon-comment"></span> Tweets</a></li>
    <li role="presentation"><a href="/rate_limits"><span class="glyphicon glyphicon-signal" aria-hidden="true"></span> Rate Limits</a></li>
    <li role="presentation"><a href="/refresh"><span class="glyphicon glyphicon-refresh" aria-hidden="true"></span> Refresh</a></li>
  </ul>
  
    
  
  
  <h3>Papers</h3>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1609.02353" target="_blank">9-1-1 DDoS: Threat, Analysis and Mitigation</a> <a href="http://arxiv.org/pdf/1609.02353" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Mordechai Guri, Yisroel Mirsky, Yuval Elovici</i>
    <p>  The 911 emergency service belongs to one of the 16 critical infrastructure
sectors in the United States. Distributed denial of service (DDoS) attacks
launched from a mobile phone botnet pose a significant threat to the
availability of this vital service. In this paper we show how attackers can
exploit the cellular network protocols in order to launch an anonymized DDoS
attack on 911. The current FCC regulations require that all emergency calls be
immediately routed regardless of the caller&#39;s identifiers (e.g., IMSI and
IMEI). A rootkit placed within the baseband firmware of a mobile phone can mask
and randomize all cellular identifiers, causing the device to have no genuine
identification within the cellular network. Such anonymized phones can issue
repeated emergency calls that cannot be blocked by the network or the emergency
call centers, technically or legally. We explore the 911 infrastructure and
discuss why it is susceptible to this kind of attack. We then implement
different forms of the attack and test our implementation on a small cellular
network. Finally, we simulate and analyze anonymous attacks on a model of
current 911 infrastructure in order to measure the severity of their impact. We
found that with less than 6K bots (or $100K hardware), attackers can block
emergency services in an entire state (e.g., North Carolina) for days. We
believe that this paper will assist the respective organizations, lawmakers,
and security professionals in understanding the scope of this issue in order to
prevent possible 911-DDoS attacks in the future.
</p>
    <p>Captured tweets and retweets: 10</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/smerity/status/774763166194475009"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1609.02087" target="_blank">Clearing the Skies: A deep network architecture for single-image rain
  removal</a> <a href="http://arxiv.org/pdf/1609.02087" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Xueyang Fu, Jiabin Huang, Xinghao Ding, Yinghao Liao, John Paisley</i>
    <p>  We introduce a deep network architecture called DerainNet for removing rain
streaks from an image. Based on the deep convolutional neural network (CNN), we
directly learn the mapping relationship between rainy and clean image detail
layers from data. Because we do not possess the ground truth corresponding to
real-world rainy images, we synthesize images with rain for training. To
effectively and efficiently train the network, different with common strategies
that roughly increase depth or breadth of network, we utilize some image
processing domain knowledge to modify the objective function. Specifically, we
train our DerainNet on the detail layer rather than the image domain. Better
results can be obtained under the same net architecture. Though DerainNet is
trained on synthetic data, we still find that the learned network is very
effective on real-world images for testing. Moreover, we augment the CNN
framework with image enhancement to significantly improve the visual results.
Compared with state-of-the- art single image de-rain methods, our method has
better rain removal and much faster computation time after network training.
</p>
    <p>Captured tweets and retweets: 12</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/alexjc/status/773791610375143424"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1609.01704" target="_blank">Hierarchical Multiscale Recurrent Neural Networks</a> <a href="http://arxiv.org/pdf/1609.01704" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Junyoung Chung, Sungjin Ahn, Yoshua Bengio</i>
    <p>  Learning both hierarchical and temporal representation has been among the
long-standing challenges of recurrent neural networks. Multiscale recurrent
neural networks have been considered as a promising approach to resolve this
issue, yet there has been a lack of empirical evidence showing that this type
of models can actually capture the temporal dependencies by discovering the
latent hierarchical structure of the sequence. In this paper, we propose a
novel multiscale approach, called the hierarchical multiscale recurrent neural
networks, which can capture the latent hierarchical structure in the sequence
by encoding the temporal dependencies with different timescales using a novel
update mechanism. We show some evidence that our proposed multiscale
architecture can discover underlying hierarchical structure in the sequences
without using explicit boundary information. We evaluate our proposed model on
character-level language modelling and handwriting sequence modelling.
</p>
    <p>Captured tweets and retweets: 73</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/nandodf/status/773422416571883524"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/kastnerkyle/status/773414482899046401"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1609.00408" target="_blank">Defeating Image Obfuscation with Deep Learning</a> <a href="http://arxiv.org/pdf/1609.00408" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Richard McPherson, Reza Shokri, Vitaly Shmatikov</i>
    <p>  We demonstrate that modern image recognition methods based on artificial
neural networks can recover hidden information from images protected by various
forms of obfuscation. The obfuscation techniques considered in this paper are
mosaicing (also known as pixelation), blurring (as used by YouTube), and P3, a
recently proposed system for privacy-preserving photo sharing that encrypts the
significant JPEG coefficients to make images unrecognizable by humans. We
empirically show how to train artificial neural networks to successfully
identify faces and recognize objects and handwritten digits even if the images
are protected using any of the above obfuscation techniques.
</p>
    <p>Captured tweets and retweets: 78</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/alexjc/status/772721415317585920"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1609.00150" target="_blank">Reward Augmented Maximum Likelihood for Neural Structured Prediction</a> <a href="http://arxiv.org/pdf/1609.00150" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Mohammad Norouzi, Samy Bengio, Zhifeng Chen, Navdeep Jaitly, Mike Schuster, Yonghui Wu, Dale Schuurmans</i>
    <p>  A key problem in structured output prediction is direct optimization of the
task reward function that matters for test evaluation. This paper presents a
simple and computationally efficient approach to incorporate task reward into a
maximum likelihood framework. We establish a connection between the
log-likelihood and regularized expected reward objectives, showing that at a
zero temperature, they are approximately equivalent in the vicinity of the
optimal solution. We show that optimal regularized expected reward is achieved
when the conditional distribution of the outputs given the inputs is
proportional to their exponentiated (temperature adjusted) rewards. Based on
this observation, we optimize conditional log-probability of edited outputs
that are sampled proportionally to their scaled exponentiated reward. We apply
this framework to optimize edit distance in the output label space. Experiments
on speech recognition and machine translation for neural sequence to sequence
models show notable improvements over a maximum likelihood baseline by using
edit distance augmented maximum likelihood.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/j_gauthier/status/773364397192261633"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1609.00037" target="_blank">Good Enough Practices in Scientific Computing</a> <a href="http://arxiv.org/pdf/1609.00037" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Greg Wilson, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, Tracy K. Teal</i>
    <p>  We present a set of computing tools and techniques that every researcher can
and should adopt. These recommendations synthesize inspiration from our own
work, from the experiences of the thousands of people who have taken part in
Software Carpentry and Data Carpentry workshops over the past six years, and
from a variety of other guides. Unlike some other guides, our recommendations
are aimed specifically at people who are new to research computing.
</p>
    <p>Captured tweets and retweets: 23</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/deliprao/status/773221533846122496"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.08868" target="_blank">Demographic Dialectal Variation in Social Media: A Case Study of
  African-American English</a> <a href="http://arxiv.org/pdf/1608.08868" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Su Lin Blodgett, Lisa Green, Brendan O&#39;Connor</i>
    <p>  Though dialectal language is increasingly abundant on social media, few
resources exist for developing NLP tools to handle such language. We conduct a
case study of dialectal language in online conversational text by investigating
African-American English (AAE) on Twitter. We propose a distantly supervised
model to identify AAE-like language from demographics associated with
geo-located messages, and we verify that this language follows well-known AAE
linguistic phenomena. In addition, we analyze the quality of existing language
identification and dependency parsing tools on AAE-like text, demonstrating
that they perform poorly on such text compared to text associated with white
speakers. We also provide an ensemble classifier for language identification
which eliminates this disparity and release a new corpus of tweets containing
AAE-like language.
</p>
    <p>Captured tweets and retweets: 4</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/stanfordnlp/status/773535701841260544"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/brendan642/status/773933656050130949"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.08225" target="_blank">Why does deep and cheap learning work so well?</a> <a href="http://arxiv.org/pdf/1608.08225" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Henry W. Lin, Max Tegmark</i>
    <p>  We show how the success of deep learning depends not only on mathematics but
also on physics: although well-known mathematical theorems guarantee that
neural networks can approximate arbitrary functions well, the class of
functions of practical interest can be approximated through &#34;cheap learning&#34;
with exponentially fewer parameters than generic ones, because they have
simplifying properties tracing back to the laws of physics. The exceptional
simplicity of physics-based functions hinges on properties such as symmetry,
locality, compositionality and polynomial log-probability, and we explore how
these properties translate into exceptionally simple neural networks
approximating both natural phenomena such as images and abstract
representations thereof such as drawings. We further argue that when the
statistical process generating the data is of a certain hierarchical form
prevalent in physics and machine-learning, a deep neural network can be more
efficient than a shallow one. We formalize these claims using information
theory and discuss the relation to renormalization group procedures. Various
&#34;no-flattening theorems&#34; show when these efficient deep networks cannot be
accurately approximated by shallow ones without efficiency loss - even for
linear networks.
</p>
    <p>Captured tweets and retweets: 8</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/deliprao/status/770788372491210752"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/petewarden/status/771759924095377408"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/janexwang/status/771800321127505920"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.07905" target="_blank">Machine Comprehension Using Match-LSTM and Answer Pointer</a> <a href="http://arxiv.org/pdf/1608.07905" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Shuohang Wang, Jing Jiang</i>
    <p>  Machine comprehension of text is an important problem in natural language
processing. A recently released dataset, the Stanford Question Answering
Dataset (SQuAD), offers a large number of real questions and their answers
created by humans through crowdsourcing. SQuAD provides a challenging testbed
for evaluating machine comprehension algorithms, partly because compared with
previous datasets, in SQuAD the answers do not come from a small set of
candidate answers and they have variable lengths. We propose an end-to-end
neural architecture for the task. The architecture is based on match-LSTM, a
model we proposed previously for textual entailment, and Pointer Net, a
sequence-to-sequence model proposed by Vinyals et al.(2015) to constrain the
output tokens to be from the input sequences. We propose two ways of using
Pointer Net for our task. Our experiments show that both of our two models
substantially outperform the best results obtained by Rajpurkar et al.(2016)
using logistic regression and manually crafted features.
</p>
    <p>Captured tweets and retweets: 20</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/fchollet/status/770750317596119041"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/stanfordnlp/status/772445357489819648"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.07068" target="_blank">Title Generation for User Generated Videos</a> <a href="http://arxiv.org/pdf/1608.07068" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Kuo-Hao Zeng, Tseng-Hung Chen, Juan Carlos Niebles, Min Sun</i>
    <p>  A great video title describes the most salient event compactly and captures
the viewer&#39;s attention. In contrast, video captioning tends to generate
sentences that describe the video as a whole. Although generating a video title
automatically is a very useful task, it is much less addressed than video
captioning. We address video title generation for the first time by proposing
two methods that extend state-of-the-art video captioners to this new task.
First, we make video captioners highlight sensitive by priming them with a
highlight detector. Our framework allows for jointly training a model for title
generation and video highlight localization. Second, we induce high sentence
diversity in video captioners, so that the generated titles are also diverse
and catchy. This means that a large number of sentences might be required to
learn the sentence structure of titles. Hence, we propose a novel sentence
augmentation method to train a captioner with additional sentence-only examples
that come without corresponding videos. We collected a large-scale Video Titles
in the Wild (VTW) dataset of 18100 automatically crawled user-generated videos
and titles. On VTW, our methods consistently improve title prediction accuracy,
and achieve the best performance in both automatic and human evaluation.
Finally, our sentence augmentation method also outperforms the baselines on the
M-VAD dataset.
</p>
    <p>Captured tweets and retweets: 3</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/alexjc/status/769076592396083202"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.06197" target="_blank">CrowdNet: A Deep Convolutional Network for Dense Crowd Counting</a> <a href="http://arxiv.org/pdf/1608.06197" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Lokesh Boominathan, Srinivas S S Kruthiventi, R. Venkatesh Babu</i>
    <p>  Our work proposes a novel deep learning framework for estimating crowd
density from static images of highly dense crowds. We use a combination of deep
and shallow, fully convolutional networks to predict the density map for a
given crowd image. Such a combination is used for effectively capturing both
the high-level semantic information (face/body detectors) and the low-level
features (blob detectors), that are necessary for crowd counting under large
scale variations. As most crowd datasets have limited training samples (&lt;100
images) and deep learning based approaches require large amounts of training
data, we perform multi-scale data augmentation. Augmenting the training samples
in such a manner helps in guiding the CNN to learn scale invariant
representations. Our method is tested on the challenging UCF_CC_50 dataset, and
shown to outperform the state of the art methods.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/deep_hub/status/768163257488121856"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.05343" target="_blank">Decoupled Neural Interfaces using Synthetic Gradients</a> <a href="http://arxiv.org/pdf/1608.05343" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Max Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, Koray Kavukcuoglu</i>
    <p>  Training directed neural networks typically requires forward-propagating data
through a computation graph, followed by backpropagating error signal, to
produce weight updates. All layers, or more generally, modules, of the network
are therefore locked, in the sense that they must wait for the remainder of the
network to execute forwards and propagate error backwards before they can be
updated. In this work we break this constraint by decoupling modules by
introducing a model of the future computation of the network graph. These
models predict what the result of the modelled subgraph will produce using only
local information. In particular we focus on modelling error gradients: by
using the modelled synthetic gradient in place of true backpropagated error
gradients we decouple subgraphs, and can update them independently and
asynchronously i.e. we realise decoupled neural interfaces. We show results for
feed-forward models, where every layer is trained asynchronously, recurrent
neural networks (RNNs) where predicting one&#39;s future gradient extends the time
over which the RNN can effectively model, and also a hierarchical RNN system
with ticking at different timescales. Finally, we demonstrate that in addition
to predicting gradients, the same framework can be used to predict inputs,
resulting in models which are decoupled in both the forward and backwards pass
-- amounting to independent networks which co-learn such that they can be
composed into a single functioning corporation.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/petewarden/status/766477174660476928"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.05148" target="_blank">Full Resolution Image Compression with Recurrent Neural Networks</a> <a href="http://arxiv.org/pdf/1608.05148" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>George Toderici, Damien Vincent, Nick Johnston, Sung Jin Hwang, David Minnen, Joel Shor, Michele Covell</i>
    <p>  This paper presents a set of full-resolution lossy image compression methods
based on neural networks. Each of the architectures we describe can provide
variable compression rates during deployment without requiring retraining of
the network: each network need only be trained once. All of our architectures
consist of a recurrent neural network (RNN)-based encoder and decoder, a
binarizer, and a neural network for entropy coding. We compare RNN types (LSTM,
associative LSTM) and introduce a new hybrid of GRU and ResNet. We also study
&#34;one-shot&#34; versus additive reconstruction architectures and introduce a new
scaled-additive framework. We compare to previous work, showing improvements of
4.3%-8.8% AUC (area under the rate-distortion curve), depending on the
perceptual metric used. As far as we know, this is the first neural network
architecture that is able to outperform JPEG at image compression across most
bitrates on the rate-distortion curve on the Kodak dataset images, with and
without the aid of entropy coding.
</p>
    <p>Captured tweets and retweets: 3</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/alexjc/status/766611397627219968"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/petewarden/status/768110248091537409"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.05081" target="_blank">Efficient Exploration for Dialog Policy Learning with Deep BBQ Networks
  \&amp; Replay Buffer Spiking</a> <a href="http://arxiv.org/pdf/1608.05081" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Zachary C. Lipton, Jianfeng Gao, Lihong Li, Xiujun Li, Faisal Ahmed, Li Deng</i>
    <p>  When rewards are sparse and efficient exploration essential, deep Q-learning
with $\epsilon$-greedy exploration tends to fail. This poses problems for
otherwise promising domains such as task-oriented dialog systems, where the
primary reward signal, indicating successful completion, typically occurs only
at the end of each episode but depends on the entire sequence of utterances. A
poor agent encounters such successful dialogs rarely, and a random agent may
never stumble upon a successful outcome in reasonable time. We present two
techniques that significantly improve the efficiency of exploration for deep
Q-learning agents in dialog systems. First, we demonstrate that exploration by
Thompson sampling, using Monte Carlo samples from a Bayes-by-Backprop neural
network, yields marked improvement over standard DQNs with Boltzmann or
$\epsilon$-greedy exploration. Second, we show that spiking the replay buffer
with a small number of successes, as are easy to harvest for dialog tasks, can
make Q-learning feasible when it might otherwise fail catastrophically.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/boredyannlecun/status/766908552552132608"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.04631" target="_blank">Neural versus Phrase-Based Machine Translation Quality: a Case Study</a> <a href="http://arxiv.org/pdf/1608.04631" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Luisa Bentivogli, Arianna Bisazza, Mauro Cettolo, Marcello Federico</i>
    <p>  Within the field of Statistical Machine Translation (SMT), the neural
approach (NMT) has recently emerged as the first technology able to challenge
the long-standing dominance of phrase-based approaches (PBMT). In particular,
at the IWSLT 2015 evaluation campaign, NMT outperformed well established
state-of-the-art PBMT systems on English-German, a language pair known to be
particularly hard because of morphology and syntactic differences. To
understand in what respects NMT provides better translation quality than PBMT,
we perform a detailed analysis of neural versus phrase-based SMT outputs,
leveraging high quality post-edits performed by professional translators on the
IWSLT data. For the first time, our analysis provides useful insights on what
linguistic phenomena are best modeled by neural models -- such as the
reordering of verbs -- while pointing out other aspects that remain to be
improved.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/stanfordnlp/status/770705158451793920"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.04428" target="_blank">TerpreT: A Probabilistic Programming Language for Program Induction</a> <a href="http://arxiv.org/pdf/1608.04428" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Alexander L. Gaunt, Marc Brockschmidt, Rishabh Singh, Nate Kushman, Pushmeet Kohli, Jonathan Taylor, Daniel Tarlow</i>
    <p>  We study machine learning formulations of inductive program synthesis; given
input-output examples, we try to synthesize source code that maps inputs to
corresponding outputs. Our aims are to develop new machine learning approaches
based on neural networks and graphical models, and to understand the
capabilities of machine learning techniques relative to traditional
alternatives, such as those based on constraint solving from the programming
languages community.
  Our key contribution is the proposal of TerpreT, a domain-specific language
for expressing program synthesis problems. TerpreT is similar to a
probabilistic programming language: a model is composed of a specification of a
program representation (declarations of random variables) and an interpreter
describing how programs map inputs to outputs (a model connecting unknowns to
observations). The inference task is to observe a set of input-output examples
and infer the underlying program. TerpreT has two main benefits. First, it
enables rapid exploration of a range of domains, program representations, and
interpreter models. Second, it separates the model specification from the
inference algorithm, allowing like-to-like comparisons between different
approaches to inference. From a single TerpreT specification we automatically
perform inference using four different back-ends. These are based on gradient
descent, linear program (LP) relaxations for graphical models, discrete
satisfiability solving, and the Sketch program synthesis system.
  We illustrate the value of TerpreT by developing several interpreter models
and performing an empirical comparison between alternative inference
algorithms. Our key empirical finding is that constraint solvers dominate the
gradient descent and LP-based formulations. We conclude with suggestions for
the machine learning community to make progress on program synthesis.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/numbercrunching/status/766027803338928129"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.04062" target="_blank">Stacked Approximated Regression Machine: A Simple Deep Learning Approach</a> <a href="http://arxiv.org/pdf/1608.04062" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Zhangyang Wang, Shiyu Chang, Qing Ling, Shuai Huang, Xia Hu, Honghui Shi, Thomas S. Huang</i>
    <p>  With the agreement of my coauthors, I Zhangyang Wang would like to withdraw
the manuscript “Stacked Approximated Regression Machine: A Simple Deep Learning
Approach”. Some experimental procedures were not included in the manuscript,
which makes a part of important claims not meaningful. In the relevant
research, I was solely responsible for carrying out the experiments; the other
coauthors joined in the discussions leading to the main algorithm.
  Please see the updated text for more details.
</p>
    <p>Captured tweets and retweets: 89</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/deliprao/status/774301347021586432"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/fchollet/status/774065138592690176"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/randal_olson/status/774276161270448132"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/deep_hub/status/771846438795882496"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/fchollet/status/771862837819867136"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/graycrawford/status/771844248929009664"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/karpathy/status/771822553903923202"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/tim_zaman/status/772029237855416320"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/dennybritz/status/774067398307553280"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.03983" target="_blank">SGDR: Stochastic Gradient Descent with Restarts</a> <a href="http://arxiv.org/pdf/1608.03983" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Ilya Loshchilov, Frank Hutter</i>
    <p>  Restart techniques are common in gradient-free optimization to deal with
multimodal functions. Partial restarts are also gaining popularity in
gradient-based optimization to improve the rate of convergence in accelerated
gradient schemes to deal with ill-conditioned functions. In this paper, we
propose a simple restart technique for stochastic gradient descent to improve
its anytime performance when training deep neural networks. We empirically
study its performance on CIFAR-10 and CIFAR-100 datasets where we demonstrate
new state-of-the-art results below 4\% and 19\%, respectively. Our source code
is available at https://github.com/loshchil/SGDR.
</p>
    <p>Captured tweets and retweets: 5</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/alexjc/status/766236305055514625"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.03819" target="_blank">DeepDiary: Automatic Caption Generation for Lifelogging Image Streams</a> <a href="http://arxiv.org/pdf/1608.03819" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Chenyou Fan, David J. Crandall</i>
    <p>  Lifelogging cameras capture everyday life from a first-person perspective,
but generate so much data that it is hard for users to browse and organize
their image collections effectively. In this paper, we propose to use automatic
image captioning algorithms to generate textual representations of these
collections. We develop and explore novel techniques based on deep learning to
generate captions for both individual images and image streams, using temporal
consistency constraints to create summaries that are both more compact and less
noisy. We evaluate our techniques with quantitative and qualitative results,
and apply captioning to an image retrieval application for finding potentially
private images. Our results suggest that our automatic captioning algorithms,
while imperfect, may work well enough to help users manage lifelogging photo
collections.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/alexjc/status/765118769849069568"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.03785" target="_blank">Compositional Distributional Cognition</a> <a href="http://arxiv.org/pdf/1608.03785" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Yaared Al-Mehairi, Bob Coecke, Martha Lewis</i>
    <p>  We accommodate the Integrated Connectionist/Symbolic Architecture (ICS) of
[32] within the categorical compositional semantics (CatCo) of [13], forming a
model of categorical compositional cognition (CatCog). This resolves intrinsic
problems with ICS such as the fact that representations inhabit an unbounded
space and that sentences with differing tree structures cannot be directly
compared. We do so in a way that makes the most of the grammatical structure
available, in contrast to strategies like circular convolution. Using the CatCo
model also allows us to make use of tools developed for CatCo such as the
representation of ambiguity and logical reasoning via density matrices,
structural meanings for words such as relative pronouns, and addressing over-
and under-extension, all of which are present in cognitive processes. Moreover
the CatCog framework is sufficiently flexible to allow for entirely different
representations of meaning, such as conceptual spaces. Interestingly, since the
CatCo model was largely inspired by categorical quantum mechanics, so is
CatCog.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/miles_brundage/status/764989658430341120"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.03665" target="_blank">Learning Structured Sparsity in Deep Neural Networks</a> <a href="http://arxiv.org/pdf/1608.03665" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, Hai Li</i>
    <p>  High demand for computation resources severely hinders deployment of
large-scale Deep Neural Networks (DNN) in resource constrained devices. In this
work, we propose a Structured Sparsity Learning (SSL) method to regularize the
structures (i.e., filters, channels, filter shapes, and layer depth) of DNNs.
SSL can: (1) learn a compact structure from a bigger DNN to reduce computation
cost; (2) obtain a hardware-friendly structured sparsity of DNN to efficiently
accelerate the DNNs evaluation. Experimental results show that SSL achieves on
average 5.1x and 3.1x speedups of convolutional layer computation of AlexNet
against CPU and GPU, respectively, with off-the-shelf libraries. These speedups
are about twice speedups of non-structured sparsity; (3) regularize the DNN
structure to improve classification accuracy. The results show that for
CIFAR-10, regularization on layer depth can reduce 20 layers of a Deep Residual
Network (ResNet) to 18 layers while improve the accuracy from 91.25% to 92.60%,
which is still slightly higher than that of original ResNet with 32 layers. For
AlexNet, structure regularization by SSL also reduces the error by around ~1%.
Open source code is in https://github.com/wenwei202/caffe/tree/scnn.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/miles_brundage/status/764986004457525248"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.03609" target="_blank">Clockwork Convnets for Video Semantic Segmentation</a> <a href="http://arxiv.org/pdf/1608.03609" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Evan Shelhamer, Kate Rakelly, Judy Hoffman, Trevor Darrell</i>
    <p>  Recent years have seen tremendous progress in still-image segmentation;
however the na\&#34;ive application of these state-of-the-art algorithms to every
video frame requires considerable computation and ignores the temporal
continuity inherent in video. We propose a video recognition framework that
relies on two key observations: 1) while pixels may change rapidly from frame
to frame, the semantic content of a scene evolves more slowly, and 2) execution
can be viewed as an aspect of architecture, yielding purpose-fit computation
schedules for networks. We define a novel family of &#34;clockwork&#34; convnets driven
by fixed or adaptive clock signals that schedule the processing of different
layers at different update rates according to their semantic stability. We
design a pipeline schedule to reduce latency for real-time recognition and a
fixed-rate schedule to reduce overall computation. Finally, we extend clockwork
scheduling to adaptive video processing by incorporating data-driven clocks
that can be tuned on unlabeled video. The accuracy and efficiency of clockwork
convnets are evaluated on the Youtube-Objects, NYUD, and Cityscapes video
datasets.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/miles_brundage/status/764986762011836416"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.02908" target="_blank">Residual Networks of Residual Networks: Multilevel Residual Networks</a> <a href="http://arxiv.org/pdf/1608.02908" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Ke Zhang, Miao Sun, Tony X. Han, Xingfang Yuan, Liru Guo, Tao Liu</i>
    <p>  Residual networks family with hundreds or even thousands of layers dominate
major image recognition tasks, but building a network by simply stacking
residual blocks inevitably limits its optimization ability. This paper proposes
a novel residual-network architecture, Residual networks of Residual networks
(RoR), to dig the optimization ability of residual networks. RoR substitutes
optimizing residual mapping of residual mapping for optimizing original
residual mapping, in particular, adding level-wise shortcut connections upon
original residual networks, to promote the learning capability of residual
networks. More importantly, RoR can be applied to various kinds of residual
networks (Pre-ResNets and WRN) and significantly boost their performance. Our
experiments demonstrate the effectiveness and versatility of RoR, where it
achieves the best performance in all residual-network-like structures. Our
RoR-3-WRN58-4 models achieve new state-of-the-art results on CIFAR-10,
CIFAR-100 and SVHN, with test errors 3.77%, 19.73% and 1.59% respectively.
These results outperform 1001-layer Pre-ResNets by 18.4% on CIFAR-10 and 13.1%
on CIFAR-100.
</p>
    <p>Captured tweets and retweets: 7</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/nandodf/status/763367917522067457"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/deep_hub/status/763423443245293568"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/ogrisel/status/763389214851346434"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/miles_brundage/status/763176263704084484"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.01230" target="_blank">Learning a Driving Simulator</a> <a href="http://arxiv.org/pdf/1608.01230" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Eder Santana, George Hotz</i>
    <p>  Comma.ai&#39;s approach to Artificial Intelligence for self-driving cars is based
on an agent that learns to clone driver behaviors and plans maneuvers by
simulating future events in the road. This paper illustrates one of our
research approaches for driving simulation. One where we learn to simulate.
Here we investigate variational autoencoders with classical and learned cost
functions using generative adversarial networks for embedding road frames.
Afterwards, we learn a transition model in the embedded space using action
conditioned Recurrent Neural Networks. We show that our approach can keep
predicting realistic looking video for several frames despite the transition
model being optimized without a cost function in the pixel space.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/hardmaru/status/763421056585478144"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.00367" target="_blank">Accelerating the Super-Resolution Convolutional Neural Network</a> <a href="http://arxiv.org/pdf/1608.00367" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Chao Dong, Chen Change Loy, Xiaoou Tang</i>
    <p>  As a successful deep model applied in image super-resolution (SR), the
Super-Resolution Convolutional Neural Network (SRCNN) has demonstrated superior
performance to the previous hand-crafted models either in speed and restoration
quality. However, the high computational cost still hinders it from practical
usage that demands real-time performance (24 fps). In this paper, we aim at
accelerating the current SRCNN, and propose a compact hourglass-shape CNN
structure for faster and better SR. We re-design the SRCNN structure mainly in
three aspects. First, we introduce a deconvolution layer at the end of the
network, then the mapping is learned directly from the original low-resolution
image (without interpolation) to the high-resolution one. Second, we
reformulate the mapping layer by shrinking the input feature dimension before
mapping and expanding back afterwards. Third, we adopt smaller filter sizes but
more mapping layers. The proposed model achieves a speed up of more than 40
times with even superior restoration quality. Further, we present the parameter
settings that can achieve real-time performance on a generic CPU while still
maintaining good performance. A corresponding transfer strategy is also
proposed for fast training and testing across different upscaling factors.
</p>
    <p>Captured tweets and retweets: 3</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/alexjc/status/760425562632577024"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1608.00318" target="_blank">A Neural Knowledge Language Model</a> <a href="http://arxiv.org/pdf/1608.00318" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Sungjin Ahn, Heeyoul Choi, Tanel Pärnamaa, Yoshua Bengio</i>
    <p>  Communicating knowledge is a primary purpose of language. However, current
language models have significant limitations in their ability to encode or
decode knowledge. This is mainly because they acquire knowledge based on
statistical co-occurrences, even if most of the knowledge words are rarely
observed named entities. In this paper, we propose a Neural Knowledge Language
Model (NKLM) which combines symbolic knowledge provided by knowledge graphs
with RNN language models. At each time step, the model predicts a fact on which
the observed word is supposed to be based. Then, a word is either generated
from the vocabulary or copied from the knowledge graph. We train and test the
model on a new dataset, WikiFacts. In experiments, we show that the NKLM
significantly improves the perplexity while generating a much smaller number of
unknown words. In addition, we demonstrate that the sampled descriptions
include named entities which were used to be the unknown words in RNN language
models.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/mat_kelcey/status/762391160467763200"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1607.08022" target="_blank">Instance Normalization: The Missing Ingredient for Fast Stylization</a> <a href="http://arxiv.org/pdf/1607.08022" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky</i>
    <p>  It this paper we revisit the fast stylization method introduced in Ulyanov
et. al. (2016). We show how a small change in the stylization architecture
results in a significant qualitative improvement in the generated images. The
change is limited to swapping batch normalization with instance normalization,
and to apply the latter both at training and testing times. The resulting
method can be used to train high-performance architectures for real-time image
generation. The code will be made available at
https://github.com/DmitryUlyanov/texture_nets.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/dmitryulyanovml/status/758632243124432896"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1607.07195" target="_blank">Higher-Order Factorization Machines</a> <a href="http://arxiv.org/pdf/1607.07195" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Mathieu Blondel, Akinori Fujino, Naonori Ueda, Masakazu Ishihata</i>
    <p>  Factorization machines (FMs) are a supervised learning approach that can use
second-order feature combinations even when the data is very high-dimensional.
Unfortunately, despite increasing interest in FMs, there exists to date no
efficient training algorithm for higher-order FMs (HOFMs). In this paper, we
present the first generic yet efficient algorithms for training arbitrary-order
HOFMs. We also present new variants of HOFMs with shared parameters, which
greatly reduce model size and prediction times while maintaining similar
accuracy. We demonstrate the proposed approaches on four different link
prediction tasks.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/mblondel_ml/status/764089614575427584"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1607.06450" target="_blank">Layer Normalization</a> <a href="http://arxiv.org/pdf/1607.06450" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton</i>
    <p>  Training state-of-the-art, deep neural networks is computationally expensive.
One way to reduce the training time is to normalize the activities of the
neurons. A recently introduced technique called batch normalization uses the
distribution of the summed input to a neuron over a mini-batch of training
cases to compute a mean and variance which are then used to normalize the
summed input to that neuron on each training case. This significantly reduces
the training time in feed-forward neural networks. However, the effect of batch
normalization is dependent on the mini-batch size and it is not obvious how to
apply it to recurrent neural networks. In this paper, we transpose batch
normalization into layer normalization by computing the mean and variance used
for normalization from all of the summed inputs to the neurons in a layer on a
single training case. Like batch normalization, we also give each neuron its
own adaptive bias and gain which are applied after the normalization but before
the non-linearity. Unlike batch normalization, layer normalization performs
exactly the same computation at training and test times. It is also
straightforward to apply to recurrent neural networks by computing the
normalization statistics separately at each time step. Layer normalization is
very effective at stabilizing the hidden state dynamics in recurrent networks.
Empirically, we show that layer normalization can substantially reduce the
training time compared with previously published techniques.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/richardsocher/status/756359202411520000"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1607.05691" target="_blank">Information-theoretical label embeddings for large-scale image
  classification</a> <a href="http://arxiv.org/pdf/1607.05691" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>François Chollet</i>
    <p>  We present a method for training multi-label, massively multi-class image
classification models, that is faster and more accurate than supervision via a
sigmoid cross-entropy loss (logistic regression). Our method consists in
embedding high-dimensional sparse labels onto a lower-dimensional dense sphere
of unit-normed vectors, and treating the classification problem as a cosine
proximity regression problem on this sphere. We test our method on a dataset of
300 million high-resolution images with 17,000 labels, where it yields
considerably faster convergence, as well as a 7% higher mean average precision
compared to logistic regression.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/fchollet/status/755792045567008769"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1607.00133" target="_blank">Deep Learning with Differential Privacy</a> <a href="http://arxiv.org/pdf/1607.00133" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Martín Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, Li Zhang</i>
    <p>  Machine learning techniques based on neural networks are achieving remarkable
results in a wide variety of domains. Often, the training of models requires
large, representative datasets, which may be crowdsourced and contain sensitive
information. The models should not expose private information in these
datasets. Addressing this goal, we develop new algorithmic techniques for
learning and a refined analysis of privacy costs within the framework of
differential privacy. Our implementation and experiments demonstrate that we
can train deep neural networks with non-convex objectives, under a modest
privacy budget, and at a manageable cost in software complexity, training
efficiency, and model quality.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/jackclarksf/status/768750895681134592"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1606.09375" target="_blank">Convolutional Neural Networks on Graphs with Fast Localized Spectral
  Filtering</a> <a href="http://arxiv.org/pdf/1606.09375" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Michaël Defferrard, Xavier Bresson, Pierre Vandergheynst</i>
    <p>  Convolutional neural networks (CNNs) have greatly improved state-of-the-art
performances in a number of fields, notably computer vision and natural
language processing. In this work, we are interested in generalizing the
formulation of CNNs from low-dimensional regular Euclidean domains, where
images (2D), videos (3D) and audios (1D) are represented, to high-dimensional
irregular domains such as social networks or biological networks represented by
graphs. This paper introduces a formulation of CNNs on graphs in the context of
spectral graph theory. We borrow the fundamental tools from the emerging field
of signal processing on graphs, which provides the necessary mathematical
background and efficient numerical schemes to design localized graph filters
efficient to learn and evaluate. As a matter of fact, we introduce the first
technique that offers the same computational complexity than standard CNNs,
while being universal to any graph structure. Numerical experiments on MNIST
and 20NEWS demonstrate the ability of this novel deep learning system to learn
local, stationary, and compositional features on graphs, as long as the graph
is well-constructed.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/_onionesque/status/764536495743000576"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1606.06724" target="_blank">Tagger: Deep Unsupervised Perceptual Grouping</a> <a href="http://arxiv.org/pdf/1606.06724" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Klaus Greff, Antti Rasmus, Mathias Berglund, Tele Hotloo Hao, Jürgen Schmidhuber, Harri Valpola</i>
    <p>  We present a framework for efficient perceptual inference that explicitly
reasons about the segmentation of its inputs and features. Rather than being
trained for any specific segmentation, our framework learns the grouping
process in an unsupervised manner or alongside any supervised task. By
enriching the representations of a neural network, we enable it to group the
representations of different objects in an iterative manner. By allowing the
system to amortize the iterative inference of the groupings, we achieve very
fast convergence. In contrast to many other recently proposed methods for
addressing multi-object scenes, our system does not assume the inputs to be
images and can therefore directly handle other modalities. For multi-digit
classification of very cluttered images that require texture segmentation, our
method offers improved classification performance over convolutional networks
despite being fully connected. Furthermore, we observe that our system greatly
improves on the semi-supervised result of a baseline Ladder network on our
dataset, indicating that segmentation can also improve sample efficiency.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/nandodf/status/745965490305114112"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1606.06565" target="_blank">Concrete Problems in AI Safety</a> <a href="http://arxiv.org/pdf/1606.06565" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, Dan Mané</i>
    <p>  Rapid progress in machine learning and artificial intelligence (AI) has
brought increasing attention to the potential impacts of AI technologies on
society. In this paper we discuss one such potential impact: the problem of
accidents in machine learning systems, defined as unintended and harmful
behavior that may emerge from poor design of real-world AI systems. We present
a list of five practical research problems related to accident risk,
categorized according to whether the problem originates from having the wrong
objective function (&#34;avoiding side effects&#34; and &#34;avoiding reward hacking&#34;), an
objective function that is too expensive to evaluate frequently (&#34;scalable
supervision&#34;), or undesirable behavior during the learning process (&#34;safe
exploration&#34; and &#34;distributional shift&#34;). We review previous work in these
areas as well as suggesting research directions with a focus on relevance to
cutting-edge AI systems. Finally, we consider the high-level question of how to
think most productively about the safety of forward-looking applications of AI.
</p>
    <p>Captured tweets and retweets: 3</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/hardmaru/status/745473427620823044"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/ch402/status/745417363642716162"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1606.06539" target="_blank">Drawing and Recognizing Chinese Characters with Recurrent Neural Network</a> <a href="http://arxiv.org/pdf/1606.06539" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Xu-Yao Zhang, Fei Yin, Yan-Ming Zhang, Cheng-Lin Liu, Yoshua Bengio</i>
    <p>  Recent deep learning based approaches have achieved great success on
handwriting recognition. Chinese characters are among the most widely adopted
writing systems in the world. Previous research has mainly focused on
recognizing handwritten Chinese characters. However, recognition is only one
aspect for understanding a language, another challenging and interesting task
is to teach a machine to automatically write (pictographic) Chinese characters.
In this paper, we propose a framework by using the recurrent neural network
(RNN) as both a discriminative model for recognizing Chinese characters and a
generative model for drawing (generating) Chinese characters. To recognize
Chinese characters, previous methods usually adopt the convolutional neural
network (CNN) models which require transforming the online handwriting
trajectory into image-like representations. Instead, our RNN based approach is
an end-to-end system which directly deals with the sequential structure and
does not require any domain-specific knowledge. With the RNN system (combining
an LSTM and GRU), state-of-the-art performance can be achieved on the
ICDAR-2013 competition database. Furthermore, under the RNN framework, a
conditional generative model with character embedding is proposed for
automatically drawing recognizable Chinese characters. The generated characters
(in vector format) are human-readable and also can be recognized by the
discriminative RNN model with high accuracy. Experimental results verify the
effectiveness of using RNNs as both generative and discriminative models for
the tasks of drawing and recognizing Chinese characters.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/hardmaru/status/762787084109164544"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1606.06160" target="_blank">DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low
  Bitwidth Gradients</a> <a href="http://arxiv.org/pdf/1606.06160" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, Yuheng Zou</i>
    <p>  We propose DoReFa-Net, a method to train convolutional neural networks that
have low bitwidth weights and activations using low bitwidth parameter
gradients. In particular, during backward pass, parameter gradients are
stochastically quantized to low bitwidth numbers before being propagated to
convolutional layers. As convolutions during forward/backward passes can now
operate on low bitwidth weights and activations/gradients respectively,
DoReFa-Net can use bit convolution kernels to accelerate both training and
inference. Moreover, as bit convolutions can be efficiently implemented on CPU,
FPGA, ASIC and GPU, DoReFa-Net opens the way to accelerate training of low
bitwidth neural network on these hardware. Our experiments on SVHN and ImageNet
datasets prove that DoReFa-Net can achieve comparable prediction accuracy as
32-bit counterparts. For example, a DoReFa-Net derived from AlexNet that has
1-bit weights, 2-bit activations, can be trained from scratch using 6-bit
gradients to get 46.1\% top-1 accuracy on ImageNet validation set. The
DoReFa-Net AlexNet model is released publicly.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/culurciello/status/747506476793495553"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1606.06121" target="_blank">Quantifying and Reducing Stereotypes in Word Embeddings</a> <a href="http://arxiv.org/pdf/1606.06121" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, Adam Kalai</i>
    <p>  Machine learning algorithms are optimized to model statistical properties of
the training data. If the input data reflects stereotypes and biases of the
broader society, then the output of the learning algorithm also captures these
stereotypes. In this paper, we initiate the study of gender stereotypes in {\em
word embedding}, a popular framework to represent text data. As their use
becomes increasingly common, applications can inadvertently amplify unwanted
stereotypes. We show across multiple datasets that the embeddings contain
significant gender stereotypes, especially with regard to professions. We
created a novel gender analogy task and combined it with crowdsourcing to
systematically quantify the gender bias in a given embedding. We developed an
efficient algorithm that reduces gender stereotype using just a handful of
training examples while preserving the useful geometric properties of the
embedding. We evaluated our algorithm on several metrics. While we focus on
male/female stereotypes, our framework may be applicable to other types of
embedding biases.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/chrmanning/status/768469453780623360"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1606.05328" target="_blank">Conditional Image Generation with PixelCNN Decoders</a> <a href="http://arxiv.org/pdf/1606.05328" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, Koray Kavukcuoglu</i>
    <p>  This work explores conditional image generation with a new image density
model based on the PixelCNN architecture. The model can be conditioned on any
vector, including descriptive labels or tags, or latent embeddings created by
other networks. When conditioned on class labels from the ImageNet database,
the model is able to generate diverse, realistic scenes representing distinct
animals, objects, landscapes and structures. When conditioned on an embedding
produced by a convolutional network given a single image of an unseen face, it
generates a variety of new portraits of the same person with different facial
expressions, poses and lighting conditions. We also show that conditional
PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally,
the gated convolutional layers in the proposed model improve the log-likelihood
of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet,
with greatly reduced computational cost.
</p>
    <p>Captured tweets and retweets: 3</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/petewarden/status/744692488657575937"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/lemonodor/status/743687707960893441"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1606.04695" target="_blank">Strategic Attentive Writer for Learning Macro-Actions</a> <a href="http://arxiv.org/pdf/1606.04695" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i> Alexander,  Vezhnevets, Volodymyr Mnih, John Agapiou, Simon Osindero, Alex Graves, Oriol Vinyals, Koray Kavukcuoglu</i>
    <p>  We present a novel deep recurrent neural network architecture that learns to
build implicit plans in an end-to-end manner by purely interacting with an
environment in reinforcement learning setting. The network builds an internal
plan, which is continuously updated upon observation of the next input from the
environment. It can also partition this internal representation into contiguous
sub- sequences by learning for how long the plan can be committed to - i.e.
followed without re-planing. Combining these properties, the proposed model,
dubbed STRategic Attentive Writer (STRAW) can learn high-level, temporally
abstracted macro- actions of varying lengths that are solely learnt from data
without any prior information. These macro-actions enable both structured
exploration and economic computation. We experimentally demonstrate that STRAW
delivers strong improvements on several ATARI games by employing temporally
extended planning strategies (e.g. Ms. Pacman and Frostbite). It is at the same
time a general algorithm that can be applied on any sequence data. To that end,
we also show that when trained on text prediction task, STRAW naturally
predicts frequent n-grams (instead of macro-actions), demonstrating the
generality of the approach.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/deepmindai/status/743379761452425216"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1606.04671" target="_blank">Progressive Neural Networks</a> <a href="http://arxiv.org/pdf/1606.04671" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, Raia Hadsell</i>
    <p>  Learning to solve complex sequences of tasks--while both leveraging transfer
and avoiding catastrophic forgetting--remains a key obstacle to achieving
human-level intelligence. The progressive networks approach represents a step
forward in this direction: they are immune to forgetting and can leverage prior
knowledge via lateral connections to previously learned features. We evaluate
this architecture extensively on a wide variety of reinforcement learning tasks
(Atari and 3D maze games), and show that it outperforms common baselines based
on pretraining and finetuning. Using a novel sensitivity measure, we
demonstrate that transfer occurs at both low-level sensory and high-level
control layers of the learned policy.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/deepmindai/status/743253796202156033"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1606.04474" target="_blank">Learning to learn by gradient descent by gradient descent</a> <a href="http://arxiv.org/pdf/1606.04474" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W. Hoffman, David Pfau, Tom Schaul, Nando de Freitas</i>
    <p>  The move from hand-designed features to learned features in machine learning
has been wildly successful. In spite of this, optimization algorithms are still
designed by hand. In this paper we show how the design of an optimization
algorithm can be cast as a learning problem, allowing the algorithm to learn to
exploit structure in the problems of interest in an automatic way. Our learned
algorithms, implemented by LSTMs, outperform generic, hand-designed competitors
on the tasks for which they are trained, and also generalize well to new tasks
with similar structure. We demonstrate this on a number of tasks, including
simple convex problems, training neural networks, and styling images with
neural art.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/deepmindai/status/742887450981376000"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1606.04442" target="_blank">DeepMath - Deep Sequence Models for Premise Selection</a> <a href="http://arxiv.org/pdf/1606.04442" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Alex A. Alemi, Francois Chollet, Geoffrey Irving, Christian Szegedy, Josef Urban</i>
    <p>  We study the effectiveness of neural sequence models for premise selection in
automated theorem proving, one of the main bottlenecks in the formalization of
mathematics. We propose a two stage approach for this task that yields good
results for the premise selection task on the Mizar corpus while avoiding the
hand-engineered features of existing state-of-the-art models. To our knowledge,
this is the first time deep learning has been applied to theorem proving.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/fchollet/status/742875527086628865"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1606.04080" target="_blank">Matching Networks for One Shot Learning</a> <a href="http://arxiv.org/pdf/1606.04080" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, Daan Wierstra</i>
    <p>  Learning from a few examples remains a key challenge in machine learning.
Despite recent advances in important domains such as vision and language, the
standard supervised deep learning paradigm does not offer a satisfactory
solution for learning new concepts rapidly from little data. In this work, we
employ ideas from metric learning based on deep neural features and from recent
advances that augment neural networks with external memories. Our framework
learns a network that maps a small labelled support set and an unlabelled
example to its label, obviating the need for fine-tuning to adapt to new class
types. We then define one-shot learning problems on vision (using Omniglot,
ImageNet) and language tasks. Our algorithm improves one-shot accuracy on
ImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to
competing approaches. We also demonstrate the usefulness of the same model on
language modeling by introducing a one-shot task on the Penn Treebank.
</p>
    <p>Captured tweets and retweets: 3</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/oriolvinyalsml/status/742557162493386752"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1606.03657" target="_blank">InfoGAN: Interpretable Representation Learning by Information Maximizing
  Generative Adversarial Nets</a> <a href="http://arxiv.org/pdf/1606.03657" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, Pieter Abbeel</i>
    <p>  This paper describes InfoGAN, an information-theoretic extension to the
Generative Adversarial Network that is able to learn disentangled
representations in a completely unsupervised manner. InfoGAN is a generative
adversarial network that also maximizes the mutual information between a small
subset of the latent variables and the observation. We derive a lower bound to
the mutual information objective that can be optimized efficiently, and show
that our training procedure can be interpreted as a variation of the Wake-Sleep
algorithm. Specifically, InfoGAN successfully disentangles writing styles from
digit shapes on the MNIST dataset, pose from lighting of 3D rendered images,
and background digits from the central digit on the SVHN dataset. It also
discovers visual concepts that include hair styles, presence/absence of
eyeglasses, and emotions on the CelebA face dataset. Experiments show that
InfoGAN learns interpretable representations that are competitive with
representations learned by existing fully supervised methods.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/ch402/status/742750802507223040"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1606.03498" target="_blank">Improved Techniques for Training GANs</a> <a href="http://arxiv.org/pdf/1606.03498" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen</i>
    <p>  We present a variety of new architectural features and training procedures
that we apply to the generative adversarial networks (GANs) framework. We focus
on two applications of GANs: semi-supervised learning, and the generation of
images that humans find visually realistic. Unlike most work on generative
models, our primary goal is not to train a model that assigns high likelihood
to test data, nor do we require the model to be able to learn well without
using any labels. Using our new techniques, we achieve state-of-the-art results
in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated
images are of high quality as confirmed by a visual Turing test: our model
generates MNIST samples that humans cannot distinguish from real data, and
CIFAR-10 samples that yield a human error rate of 21.3%. We also present
ImageNet samples with unprecedented resolution and show that our methods enable
the model to learn recognizable features of ImageNet classes.
</p>
    <p>Captured tweets and retweets: 3</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/mat_kelcey/status/742840044734418945"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/ch402/status/742750802507223040"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1606.02447" target="_blank">Learning Language Games through Interaction</a> <a href="http://arxiv.org/pdf/1606.02447" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Sida I. Wang, Percy Liang, Christopher D. Manning</i>
    <p>  We introduce a new language learning setting relevant to building adaptive
natural language interfaces. It is inspired by Wittgenstein&#39;s language games: a
human wishes to accomplish some task (e.g., achieving a certain configuration
of blocks), but can only communicate with a computer, who performs the actual
actions (e.g., removing all red blocks). The computer initially knows nothing
about language and therefore must learn it from scratch through interaction,
while the human adapts to the computer&#39;s capabilities. We created a game in a
blocks world and collected interactions from 100 people playing it. First, we
analyze the humans&#39; strategies, showing that using compositionality and
avoiding synonyms correlates positively with task performance. Second, we
compare computer strategies, showing how to quickly learn a semantic parsing
model from scratch, and that modeling pragmatics further accelerates learning
for successful players.
</p>
    <p>Captured tweets and retweets: 3</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/grantdelozier/status/763313602241716224"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/albertstartup/status/773006519243726848"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1606.01933" target="_blank">A Decomposable Attention Model for Natural Language Inference</a> <a href="http://arxiv.org/pdf/1606.01933" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Ankur P. Parikh, Oscar Täckström, Dipanjan Das, Jakob Uszkoreit</i>
    <p>  We propose a simple neural architecture for natural language inference. Our
approach uses attention to decompose the problem into subproblems that can be
solved separately, thus making it trivially parallelizable. On the Stanford
Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results
with almost an order of magnitude fewer parameters than previous work and
without relying on any word-order information. Adding intra-sentence attention
that takes a minimum amount of order into account yields further improvements.
</p>
    <p>Captured tweets and retweets: 33</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/yoavgo/status/772062665627795457"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1606.01868" target="_blank">Unifying Count-Based Exploration and Intrinsic Motivation</a> <a href="http://arxiv.org/pdf/1606.01868" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Marc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, Remi Munos</i>
    <p>  We consider an agent&#39;s uncertainty about its environment and the problem of
generalizing this uncertainty across observations. Specifically, we focus on
the problem of exploration in non-tabular reinforcement learning. Drawing
inspiration from the intrinsic motivation literature, we use sequential density
models to measure uncertainty, and propose a novel algorithm for deriving a
pseudo-count from an arbitrary sequential density model. This technique enables
us to generalize count-based exploration algorithms to the non-tabular case. We
apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw
pixels. We transform these pseudo-counts into intrinsic rewards and obtain
significantly improved exploration in a number of hard games, including the
infamously difficult Montezuma&#39;s Revenge.
</p>
    <p>Captured tweets and retweets: 3</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/shakir_za/status/740174043698102272"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/kastnerkyle/status/740049984096083969"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1606.01455" target="_blank">Multimodal Residual Learning for Visual QA</a> <a href="http://arxiv.org/pdf/1606.01455" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Jin-Hwa Kim, Sang-Woo Lee, Dong-Hyun Kwak, Min-Oh Heo, Jeonghee Kim, Jung-Woo Ha, Byoung-Tak Zhang</i>
    <p>  Deep neural networks continue to advance the state-of-the-art of image
recognition tasks with various methods. However, applications of these methods
to multimodality remain limited. We present Multimodal Residual Networks (MRN)
for the multimodal residual learning of visual question-answering, which
extends the idea of the deep residual learning. Unlike the deep residual
learning, MRN effectively learns the joint representation from vision and
language information. The main idea is to use element-wise multiplication for
the joint residual mappings exploiting the residual learning of the attentional
models in recent studies. Various alternative models introduced by
multimodality are explored based on our study. We achieve the state-of-the-art
results on the Visual QA dataset for both Open-Ended and Multiple-Choice tasks.
Moreover, we introduce a novel method to visualize the attention effect of the
joint representations for each learning block using back-propagation algorithm,
even though the visual features are collapsed without spatial information.
</p>
    <p>Captured tweets and retweets: 3</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/jnhwkim/status/771674699155648512"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/jnhwkim/status/771153382140579840"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/jnhwkim/status/763891103258910720"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1606.01305" target="_blank">Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations</a> <a href="http://arxiv.org/pdf/1606.01305" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>David Krueger, Tegan Maharaj, János Kramár, Mohammad Pezeshki, Nicolas Ballas, Nan Rosemary Ke, Anirudh Goyal, Yoshua Bengio, Hugo Larochelle, Aaron Courville, Chris Pal</i>
    <p>  We propose zoneout, a novel method for regularizing RNNs. At each timestep,
zoneout stochastically forces some hidden units to maintain their previous
values. Like dropout, zoneout uses random noise to train a pseudo-ensemble,
improving generalization. But by preserving instead of dropping hidden units,
gradient information and state information are more readily propagated through
time, as in feedforward stochastic depth networks. We perform an empirical
investigation of various RNN regularizers, and find encouraging results:
zoneout gives significant performance improvements across tasks, yielding
state-of-the-art results in character-level language modeling on the Penn
Treebank dataset and competitive results on word-level Penn Treebank and
permuted sequential MNIST classification tasks.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/karpathy/status/740045174634516480"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/smerity/status/771597610863857664"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1606.01167" target="_blank">How Deep is the Feature Analysis underlying Rapid Visual Categorization?</a> <a href="http://arxiv.org/pdf/1606.01167" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Sven Eberhardt, Jonah Cader, Thomas Serre</i>
    <p>  Rapid categorization paradigms have a long history in experimental
psychology: Characterized by short presentation times and speedy behavioral
responses, these tasks highlight the efficiency with which our visual system
processes natural object categories. Previous studies have shown that
feed-forward hierarchical models of the visual cortex provide a good fit to
human visual decisions. At the same time, recent work in computer vision has
demonstrated significant gains in object recognition accuracy with increasingly
deep hierarchical architectures. But it is unclear how well these models
account for human visual decisions and what they may reveal about the
underlying brain processes.
  We have conducted a large-scale psychophysics study to assess the correlation
between computational models and human participants on a rapid animal vs.
non-animal categorization task. We considered visual representations of varying
complexity by analyzing the output of different stages of processing in three
state-of-the-art deep networks. We found that recognition accuracy increases
with higher stages of visual processing (higher level stages indeed
outperforming human participants on the same task) but that human decisions
agree best with predictions from intermediate stages.
  Overall, these results suggest that human participants may rely on visual
features of intermediate complexity and that the complexity of visual
representations afforded by modern deep network models may exceed those used by
human participants during rapid categorization.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/karpathy/status/739693625945133057"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1605.09304" target="_blank">Synthesizing the preferred inputs for neurons in neural networks via
  deep generator networks</a> <a href="http://arxiv.org/pdf/1605.09304" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Anh Nguyen, Alexey Dosovitskiy, Jason Yosinski, Thomas Brox, Jeff Clune</i>
    <p>  Deep neural networks (DNNs) have demonstrated state-of-the-art results on
many pattern recognition tasks, especially vision classification problems.
Understanding the inner workings of such computational brains is both
fascinating basic science that is interesting in its own right - similar to why
we study the human brain - and will enable researchers to further improve DNNs.
One path to understanding how a neural network functions internally is to study
what each of its neurons has learned to detect. One such method is called
activation maximization (AM), which synthesizes an input (e.g. an image) that
highly activates a neuron. Here we dramatically improve the qualitative state
of the art of activation maximization by harnessing a powerful, learned prior:
a deep generator network (DGN). The algorithm (1) generates qualitatively
state-of-the-art synthetic images that look almost real, (2) reveals the
features learned by each neuron in an interpretable way, (3) generalizes well
to new datasets and somewhat well to different network architectures without
requiring the prior to be relearned, and (4) can be considered as a
high-quality generative method (in this case, by generating novel, creative,
interesting, recognizable images).
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/petewarden/status/738065771880800258"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1605.08803" target="_blank">Density estimation using Real NVP</a> <a href="http://arxiv.org/pdf/1605.08803" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Laurent Dinh, Jascha Sohl-Dickstein, Samy Bengio</i>
    <p>  Unsupervised learning of probabilistic models is a central yet challenging
problem in machine learning. Specifically, designing models with tractable
learning, sampling, inference and evaluation is crucial in solving this task.
We extend the space of such models using real-valued non-volume preserving
(real NVP) transformations, a set of powerful invertible and learnable
transformations, resulting in an unsupervised learning algorithm with exact
log-likelihood computation, exact sampling, exact inference of latent
variables, and an interpretable latent space. We demonstrate its ability to
model natural images on four datasets through sampling, log-likelihood
evaluation and latent variable manipulations.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/laurent_dinh/status/737464564812390400"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1605.08695" target="_blank">TensorFlow: A system for large-scale machine learning</a> <a href="http://arxiv.org/pdf/1605.08695" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, Xiaoqiang Zheng</i>
    <p>  TensorFlow is a machine learning system that operates at large scale and in
heterogeneous environments. TensorFlow uses dataflow graphs to represent
computation, shared state, and the operations that mutate that state. It maps
the nodes of a dataflow graph across many machines in a cluster, and within a
machine across multiple computational devices, including multicore CPUs,
general-purpose GPUs, and custom designed ASICs known as Tensor Processing
Units (TPUs). This architecture gives flexibility to the application developer:
whereas in previous &#34;parameter server&#34; designs the management of shared state
is built into the system, TensorFlow enables developers to experiment with
novel optimizations and training algorithms. TensorFlow supports a variety of
applications, with particularly strong support for training and inference on
deep neural networks. Several Google services use TensorFlow in production, we
have released it as an open-source project, and it has become widely used for
machine learning research. In this paper, we describe the TensorFlow dataflow
model in contrast to existing systems, and demonstrate the compelling
performance that TensorFlow achieves for several real-world applications.
</p>
    <p>Captured tweets and retweets: 4</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/mrry/status/759525483541245952"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/mrry/status/755554043347341312"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/mrry/status/737645093398532098"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1605.08104" target="_blank">Deep Predictive Coding Networks for Video Prediction and Unsupervised
  Learning</a> <a href="http://arxiv.org/pdf/1605.08104" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>William Lotter, Gabriel Kreiman, David Cox</i>
    <p>  While great strides have been made in using deep learning algorithms to solve
supervised learning tasks, the problem of unsupervised learning - leveraging
unlabeled examples to learn about the structure of a domain - remains a
difficult unsolved challenge. Here, we explore prediction of future frames in a
video sequence as an unsupervised learning rule for learning about the
structure of the visual world. We describe a predictive neural network
(&#34;PredNet&#34;) architecture that is inspired by the concept of &#34;predictive coding&#34;
from the neuroscience literature. These networks learn to predict future frames
in a video sequence, with each layer in the network making local predictions
and only forwarding deviations from those predictions to subsequent network
layers. We show that these networks are able to robustly learn to predict the
movement of synthetic (rendered) objects, and that in doing so, the networks
learn internal representations that are useful for decoding latent object
parameters (e.g. pose) that support object recognition with fewer training
views. We also show that these networks can scale to complex natural image
streams (car-mounted camera videos), capturing key aspects of both egocentric
movement and the movement of objects in the visual scene, and generalizing
across video datasets. These results suggest that prediction represents a
powerful framework for unsupervised learning, allowing for implicit learning of
object and scene structure.
</p>
    <p>Captured tweets and retweets: 17</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/deliprao/status/771421498573725696"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1605.07648" target="_blank">FractalNet: Ultra-Deep Neural Networks without Residuals</a> <a href="http://arxiv.org/pdf/1605.07648" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Gustav Larsson, Michael Maire, Gregory Shakhnarovich</i>
    <p>  We introduce a design strategy for neural network macro-architecture based on
self-similarity. Repeated application of a single expansion rule generates an
extremely deep network whose structural layout is precisely a truncated
fractal. Such a network contains interacting subpaths of different lengths, but
does not include any pass-through connections: every internal signal is
transformed by a filter and nonlinearity before being seen by subsequent
layers. This property stands in stark contrast to the current approach of
explicitly structuring very deep networks so that training is a residual
learning problem. Our experiments demonstrate that residual representation is
not fundamental to the success of extremely deep convolutional neural networks.
A fractal design achieves an error rate of 22.85% on CIFAR-100, matching the
state-of-the-art held by residual networks.
  Fractal networks exhibit intriguing properties beyond their high performance.
They can be regarded as a computationally efficient implicit union of
subnetworks of every depth. We explore consequences for training, touching upon
connection with student-teacher behavior, and, most importantly, demonstrating
the ability to extract high-performance fixed-depth subnetworks. To facilitate
this latter task, we develop drop-path, a natural extension of dropout, to
regularize co-adaptation of subpaths in fractal architectures. With such
regularization, fractal networks exhibit an anytime property: shallow
subnetworks provide a quick answer, while deeper subnetworks, with higher
latency, provide a more accurate answer.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/karpathy/status/737055043153580032"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1605.07110" target="_blank">Deep Learning without Poor Local Minima</a> <a href="http://arxiv.org/pdf/1605.07110" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Kenji Kawaguchi</i>
    <p>  In this paper, we prove a conjecture published in 1989 and also partially
address an open problem announced at the Conference on Learning Theory (COLT)
2015. With no unrealistic assumption, we first prove the following statements
for the squared loss function of deep linear neural networks with any depth and
any widths: 1) the function is non-convex and non-concave, 2) every local
minimum is a global minimum, 3) every critical point that is not a global
minimum is a saddle point, and 4) there exist &#34;bad&#34; saddle points (where the
Hessian has no negative eigenvalue) for the deeper networks (with more than
three layers), whereas there is no bad saddle point for the shallow networks
(with three layers). Moreover, for deep nonlinear neural networks, we prove the
same four statements via a reduction to a deep linear model under the
independence assumption adopted from recent work. As a result, we present an
instance, for which we can answer the following question: how difficult is it
to directly train a deep model in theory? It is more difficult than the
classical machine learning models (because of the non-convexity), but not too
difficult (because of the nonexistence of poor local minima). Furthermore, the
mathematically proven existence of bad saddle points for deeper models would
suggest a possible open problem. We note that even though we have advanced the
theoretical foundations of deep learning and non-convex optimization, there is
still a gap between theory and practice.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/_onionesque/status/765612230033276928"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1605.06457" target="_blank">Virtual Worlds as Proxy for Multi-Object Tracking Analysis</a> <a href="http://arxiv.org/pdf/1605.06457" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Adrien Gaidon, Qiao Wang, Yohann Cabon, Eleonora Vig</i>
    <p>  Modern computer vision algorithms typically require expensive data
acquisition and accurate manual labeling. In this work, we instead leverage the
recent progress in computer graphics to generate fully labeled, dynamic, and
photo-realistic proxy virtual worlds. We propose an efficient real-to-virtual
world cloning method, and validate our approach by building and publicly
releasing a new video dataset, called Virtual KITTI (see
http://www.xrce.xerox.com/Research-Development/Computer-Vision/Proxy-Virtual-Worlds),
automatically labeled with accurate ground truth for object detection,
tracking, scene and instance segmentation, depth, and optical flow. We provide
quantitative experimental evidence suggesting that (i) modern deep learning
algorithms pre-trained on real data behave similarly in real and virtual
worlds, and (ii) pre-training on virtual data improves performance. As the gap
between real and virtual worlds is small, virtual worlds enable measuring the
impact of various weather and imaging conditions on recognition performance,
all other things being equal. We show these factors may affect drastically
otherwise high-performing deep models for tracking.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/karpathy/status/755344585539346432"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1605.06432" target="_blank">Deep Variational Bayes Filters: Unsupervised Learning of State Space
  Models from Raw Data</a> <a href="http://arxiv.org/pdf/1605.06432" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Maximilian Karl, Maximilian Soelch, Justin Bayer, Patrick van der Smagt</i>
    <p>  We introduce Deep Variational Bayes Filters (DVBF), a new method for
unsupervised learning of latent Markovian state space models. Leveraging recent
advances in Stochastic Gradient Variational Bayes, DVBF can overcome
intractable inference distributions by means of variational inference. Thus, it
can handle highly nonlinear input data with temporal and spatial dependencies
such as image sequences without domain knowledge. Our experiments show that
enabling backpropagation through transitions enforces state space assumptions
and significantly improves information content of the latent embedding. This
also enables realistic long-term prediction.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/usuallyuseless/status/734990421571604480"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1605.06431" target="_blank">Residual Networks are Exponential Ensembles of Relatively Shallow
  Networks</a> <a href="http://arxiv.org/pdf/1605.06431" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Andreas Veit, Michael Wilber, Serge Belongie</i>
    <p>  In this work, we introduce a novel interpretation of residual networks
showing they are exponential ensembles. This observation is supported by a
large-scale lesion study that demonstrates they behave just like ensembles at
test time. Subsequently, we perform an analysis showing these ensembles mostly
consist of networks that are each relatively shallow. For example, contrary to
our expectations, most of the gradient in a residual network with 110 layers
comes from an ensemble of very short networks, i.e., only 10-34 layers deep.
This suggests that in addition to describing neural networks in terms of width
and depth, there is a third dimension: multiplicity, the size of the implicit
ensemble. Ultimately, residual networks do not resolve the vanishing gradient
problem by preserving gradient flow throughout the entire depth of the network
- rather, they avoid the problem simply by ensembling many short networks
together. This insight reveals that depth is still an open research question
and invites the exploration of the related notion of multiplicity.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/johnplattml/status/734608994841595904"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1605.06065" target="_blank">One-shot Learning with Memory-Augmented Neural Networks</a> <a href="http://arxiv.org/pdf/1605.06065" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, Timothy Lillicrap</i>
    <p>  Despite recent breakthroughs in the applications of deep neural networks, one
setting that presents a persistent challenge is that of &#34;one-shot learning.&#34;
Traditional gradient-based networks require a lot of data to learn, often
through extensive iterative training. When new data is encountered, the models
must inefficiently relearn their parameters to adequately incorporate the new
information without catastrophic interference. Architectures with augmented
memory capacities, such as Neural Turing Machines (NTMs), offer the ability to
quickly encode and retrieve new information, and hence can potentially obviate
the downsides of conventional models. Here, we demonstrate the ability of a
memory-augmented neural network to rapidly assimilate new data, and leverage
this data to make accurate predictions after only a few samples. We also
introduce a new method for accessing an external memory that focuses on memory
content, unlike previous methods that additionally use memory location-based
focusing mechanisms.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/shakir_za/status/733535890992029698"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1605.05396" target="_blank">Generative Adversarial Text to Image Synthesis</a> <a href="http://arxiv.org/pdf/1605.05396" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, Honglak Lee</i>
    <p>  Automatic synthesis of realistic images from text would be interesting and
useful, but current AI systems are still far from this goal. However, in recent
years generic and powerful recurrent neural network architectures have been
developed to learn discriminative text feature representations. Meanwhile, deep
convolutional generative adversarial networks (GANs) have begun to generate
highly compelling images of specific categories, such as faces, album covers,
and room interiors. In this work, we develop a novel deep architecture and GAN
formulation to effectively bridge these advances in text and image model- ing,
translating visual concepts from characters to pixels. We demonstrate the
capability of our model to generate plausible images of birds and flowers from
detailed text descriptions.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/ethancaballero/status/772298100132057090"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1605.05273" target="_blank">Learning Convolutional Neural Networks for Graphs</a> <a href="http://arxiv.org/pdf/1605.05273" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Mathias Niepert, Mohamed Ahmed, Konstantin Kutzkov</i>
    <p>  Numerous important problems can be framed as learning from graph data. We
propose a framework for learning convolutional neural networks for arbitrary
graphs. These graphs may be undirected, directed, and with both discrete and
continuous node and edge attributes. Analogous to image-based convolutional
networks that operate on locally connected regions of the input, we present a
general approach to extracting locally connected regions from graphs. Using
established benchmark data sets, we demonstrate that the learned feature
representations are competitive with state of the art graph kernels and that
their computation is highly efficient.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/_onionesque/status/732747325928509440"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1605.04462" target="_blank">Large-scale Analysis of Counseling Conversations: An Application of
  Natural Language Processing to Mental Health</a> <a href="http://arxiv.org/pdf/1605.04462" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Tim Althoff, Kevin Clark, Jure Leskovec</i>
    <p>  Mental illness is one of the most pressing public health issues of our time.
While counseling and psychotherapy can be effective treatments, our knowledge
about how to conduct successful counseling conversations has been limited due
to lack of large-scale data with labeled outcomes of the conversations. In this
paper, we present a large-scale, quantitative study on the discourse of
text-message-based counseling conversations. We develop a set of novel
computational discourse analysis methods to measure how various linguistic
aspects of conversations are correlated with conversation outcomes. Applying
techniques such as sequence-based conversation models, language model
comparisons, message clustering, and psycholinguistics-inspired word frequency
analyses, we discover actionable conversation strategies that are associated
with better conversation outcomes.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/miles_brundage/status/732433579918163968"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1604.07316" target="_blank">End to End Learning for Self-Driving Cars</a> <a href="http://arxiv.org/pdf/1604.07316" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, Xin Zhang, Jake Zhao, Karol Zieba</i>
    <p>  We trained a convolutional neural network (CNN) to map raw pixels from a
single front-facing camera directly to steering commands. This end-to-end
approach proved surprisingly powerful. With minimum training data from humans
the system learns to drive in traffic on local roads with or without lane
markings and on highways. It also operates in areas with unclear visual
guidance such as in parking lots and on unpaved roads.
  The system automatically learns internal representations of the necessary
processing steps such as detecting useful road features with only the human
steering angle as the training signal. We never explicitly trained it to
detect, for example, the outline of roads.
  Compared to explicit decomposition of the problem, such as lane marking
detection, path planning, and control, our end-to-end system optimizes all
processing steps simultaneously. We argue that this will eventually lead to
better performance and smaller systems. Better performance will result because
the internal components self-optimize to maximize overall system performance,
instead of optimizing human-selected intermediate criteria, e.g., lane
detection. Such criteria understandably are selected for ease of human
interpretation which doesn&#39;t automatically guarantee maximum system
performance. Smaller networks are possible because the system learns to solve
the problem with the minimal number of processing steps.
  We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX
self-driving car computer also running Torch 7 for determining where to drive.
The system operates at 30 frames per second (FPS).
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/deeplearning4j/status/725043954916429825"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1604.04004" target="_blank">Understanding How Image Quality Affects Deep Neural Networks</a> <a href="http://arxiv.org/pdf/1604.04004" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Samuel Dodge, Lina Karam</i>
    <p>  Image quality is an important practical challenge that is often overlooked in
the design of machine vision systems. Commonly, machine vision systems are
trained and tested on high quality image datasets, yet in practical
applications the input images can not be assumed to be of high quality.
Recently, deep neural networks have obtained state-of-the-art performance on
many machine vision tasks. In this paper we provide an evaluation of 4
state-of-the-art deep neural network models for image classification under
quality distortions. We consider five types of quality distortions: blur,
noise, contrast, JPEG, and JPEG2000 compression. We show that the existing
networks are susceptible to these quality distortions, particularly to blur and
noise. These results enable future work in developing deep neural networks that
are more invariant to quality distortions.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/kleinsound/status/723953721604890625"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1604.01692" target="_blank">An Ensemble Method to Produce High-Quality Word Embeddings</a> <a href="http://arxiv.org/pdf/1604.01692" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Robert Speer, Joshua Chin</i>
    <p>  A currently successful approach to computational semantics is to represent
words as embeddings in a machine-learned vector space. We present an ensemble
method that combines embeddings produced by GloVe (Pennington et al., 2014) and
word2vec (Mikolov et al., 2013) with structured knowledge from the semantic
networks ConceptNet (Speer and Havasi, 2012) and PPDB (Ganitkevitch et al.,
2013), merging their information into a common representation with a large,
multilingual vocabulary. The embeddings it produces achieve state-of-the-art
performance on many word-similarity evaluations. Its score of $\rho = .596$ on
an evaluation of rare words (Luong et al., 2013) is 16% higher than the
previous best known system.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/r_speer/status/771798224168480768"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1604.00788" target="_blank">Achieving Open Vocabulary Neural Machine Translation with Hybrid
  Word-Character Models</a> <a href="http://arxiv.org/pdf/1604.00788" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Minh-Thang Luong, Christopher D. Manning</i>
    <p>  Nearly all previous work on neural machine translation (NMT) has used quite
restricted vocabularies, perhaps with a subsequent method to patch in unknown
words. This paper presents a novel word-character solution to achieving open
vocabulary NMT. We build hybrid systems that translate mostly at the word level
and consult the character components for rare words. Our character-level
recurrent neural networks compute source word representations and recover
unknown target words when needed. The twofold advantage of such a hybrid
approach is that it is much faster and easier to train than character-based
ones; at the same time, it never produces unknown words as in the case of
word-based models. On the WMT&#39;15 English to Czech translation task, this hybrid
approach offers an addition boost of +2.1-11.4 BLEU points over models that
already handle unknown words. Our best system achieves a new state-of-the-art
result with 20.7 BLEU score. We demonstrate that our character models can
successfully learn to not only generate well-formed words for Czech, a
highly-inflected language with a very complex vocabulary, but also build
correct representations for English source words.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/lmthang/status/762680079478652928"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/lmthang/status/717199938933764097"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1604.00289" target="_blank">Building Machines That Learn and Think Like People</a> <a href="http://arxiv.org/pdf/1604.00289" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, Samuel J. Gershman</i>
    <p>  Recent progress in artificial intelligence (AI) has renewed interest in
building systems that learn and think like people. Many advances have come from
using deep neural networks trained end-to-end in tasks such as object
recognition, video games, and board games, achieving performance that equals or
even beats humans in some respects. Despite their biological inspiration and
performance achievements, these systems differ from human intelligence in
crucial ways. We review progress in cognitive science suggesting that truly
human-like learning and thinking machines will have to reach beyond current
engineering trends in both what they learn, and how they learn it.
Specifically, we argue that these machines should (a) build causal models of
the world that support explanation and understanding, rather than merely
solving pattern recognition problems; (b) ground learning in intuitive theories
of physics and psychology, to support and enrich the knowledge that is learned;
and (c) harness compositionality and learning-to-learn to rapidly acquire and
generalize knowledge to new tasks and situations. We suggest concrete
challenges and promising routes towards these goals that can combine the
strengths of recent neural network advances with more structured cognitive
models.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/karpathy/status/718251622288240640"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1603.09246" target="_blank">Unsupervised Learning of Visual Representations by Solving Jigsaw
  Puzzles</a> <a href="http://arxiv.org/pdf/1603.09246" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Mehdi Noroozi, Paolo Favaro</i>
    <p>  In this paper we study the problem of image representation learning without
human annotation. By following the principles of self-supervision, we build a
convolutional neural network (CNN) that can be trained to solve Jigsaw puzzles
as a pretext task, which requires no manual labeling, and then later repurposed
to solve object classification and detection. To maintain the compatibility
across tasks we introduce the context-free network (CFN), a siamese-ennead CNN.
The CFN takes image tiles as input and explicitly limits the receptive field
(or context) of its early processing units to one tile at a time. We show that
the CFN is a more compact version of AlexNet, but with the same semantic
learning capabilities. By training the CFN to solve Jigsaw puzzles, we learn
both a feature mapping of object parts as well as their correct spatial
arrangement. Our experimental evaluations show that the learned features
capture semantically relevant content. After training our CFN features to solve
jigsaw puzzles on the training set of the ILSRV 2012 dataset, we transfer them
via fine-tuning on the combined training and validation set of Pascal VOC 2007
for object detection (via fast RCNN) and classification. The performance of the
CFN features is 51.8% for detection and 68.6% for classification, which is the
highest among features obtained via unsupervised learning, and closing the gap
with features obtained via supervised learning (56.5% and 78.2% respectively).
In object classification the CFN features achieve 38.1% on the ILSRV 2012
validation set, after fine-tuning only the fully connected layers on the
training set.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/soumithchintala/status/722449348152455169"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1603.09025" target="_blank">Recurrent Batch Normalization</a> <a href="http://arxiv.org/pdf/1603.09025" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Tim Cooijmans, Nicolas Ballas, César Laurent, Çağlar Gülçehre, Aaron Courville</i>
    <p>  We propose a reparameterization of LSTM that brings the benefits of batch
normalization to recurrent neural networks. Whereas previous works only apply
batch normalization to the input-to-hidden transformation of RNNs, we
demonstrate that it is both possible and beneficial to batch-normalize the
hidden-to-hidden transition, thereby reducing internal covariate shift between
time steps. We evaluate our proposal on various sequential problems such as
sequence classification, language modeling and question answering. Our
empirical results show that our batch-normalized LSTM consistently leads to
faster convergence and improved generalization.
</p>
    <p>Captured tweets and retweets: 3</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/kastnerkyle/status/750766150796075008"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/chrisemoody/status/715418603978432512"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1603.08233" target="_blank">Evolution of active categorical image classification via saccadic eye
  movement</a> <a href="http://arxiv.org/pdf/1603.08233" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Randal S. Olson, Jason H. Moore, Christoph Adami</i>
    <p>  Pattern recognition and classification is a central concern for modern
information processing systems. In particular, one key challenge to image and
video classification has been that the computational cost of image processing
scales linearly with the number of pixels in the image or video. Here we
present an intelligent machine (the &#34;active categorical classifier,&#34; or ACC)
that is inspired by the saccadic movements of the eye, and is capable of
classifying images by selectively scanning only a portion of the image. We
harness evolutionary computation to optimize the ACC on the MNIST hand-written
digit classification task, and provide a proof-of-concept that the ACC works on
noisy multi-class data. We further analyze the ACC and demonstrate its ability
to classify images after viewing only a fraction of the pixels, and provide
insight on future research paths to further improve upon the ACC presented
here.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/doctorjosh/status/764643870491770885"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1603.08155" target="_blank">Perceptual Losses for Real-Time Style Transfer and Super-Resolution</a> <a href="http://arxiv.org/pdf/1603.08155" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Justin Johnson, Alexandre Alahi, Li Fei-Fei</i>
    <p>  We consider image transformation problems, where an input image is
transformed into an output image. Recent methods for such problems typically
train feed-forward convolutional neural networks using a \emph{per-pixel} loss
between the output and ground-truth images. Parallel work has shown that
high-quality images can be generated by defining and optimizing
\emph{perceptual} loss functions based on high-level features extracted from
pretrained networks. We combine the benefits of both approaches, and propose
the use of perceptual loss functions for training feed-forward networks for
image transformation tasks. We show results on image style transfer, where a
feed-forward network is trained to solve the optimization problem proposed by
Gatys et al in real-time. Compared to the optimization-based method, our
network gives similar qualitative results but is three orders of magnitude
faster. We also experiment with single-image super-resolution, where replacing
a per-pixel loss with a perceptual loss gives visually pleasing results.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/alexjc/status/772530088386715648"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1603.05118" target="_blank">Recurrent Dropout without Memory Loss</a> <a href="http://arxiv.org/pdf/1603.05118" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Stanislau Semeniuta, Aliaksei Severyn, Erhardt Barth</i>
    <p>  This paper presents a novel approach to recurrent neural network (RNN)
regularization. Differently from the widely adopted dropout method, which is
applied to \textit{forward} connections of feed-forward architectures or RNNs,
we propose to drop neurons directly in \textit{recurrent} connections in a way
that does not cause loss of long-term memory. Our approach is as easy to
implement and apply as the regular feed-forward dropout and we demonstrate its
effectiveness for Long Short-Term Memory network, the most popular type of RNN
cells. Our experiments on NLP benchmarks show consistent improvements even when
combined with conventional feed-forward dropout.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/ozan__caglayan/status/771594297523089408"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1603.05106" target="_blank">One-Shot Generalization in Deep Generative Models</a> <a href="http://arxiv.org/pdf/1603.05106" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Danilo Jimenez Rezende, Shakir Mohamed, Ivo Danihelka, Karol Gregor, Daan Wierstra</i>
    <p>  Humans have an impressive ability to reason about new concepts and
experiences from just a single example. In particular, humans have an ability
for one-shot generalization: an ability to encounter a new concept, understand
its structure, and then be able to generate compelling alternative variations
of the concept. We develop machine learning systems with this important
capacity by developing new deep generative models, models that combine the
representational power of deep learning with the inferential power of Bayesian
reasoning. We develop a class of sequential generative models that are built on
the principles of feedback and attention. These two characteristics lead to
generative models that are among the state-of-the art in density estimation and
image generation. We demonstrate the one-shot generalization ability of our
models using three tasks: unconditional sampling, generating new exemplars of a
given concept, and generating new exemplars of a family of concepts. In all
cases our models are able to generate compelling and diverse samples---having
seen new examples just once---providing an important class of general-purpose
models for one-shot machine learning.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/shakir_za/status/724340198616772610"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1603.01417" target="_blank">Dynamic Memory Networks for Visual and Textual Question Answering</a> <a href="http://arxiv.org/pdf/1603.01417" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Caiming Xiong, Stephen Merity, Richard Socher</i>
    <p>  Neural network architectures with memory and attention mechanisms exhibit
certain reasoning capabilities required for question answering. One such
architecture, the dynamic memory network (DMN), obtained high accuracy on a
variety of language tasks. However, it was not shown whether the architecture
achieves strong results for question answering when supporting facts are not
marked during training or whether it could be applied to other modalities such
as images. Based on an analysis of the DMN, we propose several improvements to
its memory and input modules. Together with these changes we introduce a novel
input module for images in order to be able to answer visual questions. Our new
DMN+ model improves the state of the art on both the Visual Question Answering
dataset and the \babi-10k text question-answering dataset without supporting
fact supervision.
</p>
    <p>Captured tweets and retweets: 7</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/richardsocher/status/724369376925749248"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/stephenpiment/status/706967564732096513"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/smerity/status/706693233972129792"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/metamindio/status/706681498531864576"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1602.07332" target="_blank">Visual Genome: Connecting Language and Vision Using Crowdsourced Dense
  Image Annotations</a> <a href="http://arxiv.org/pdf/1602.07332" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, Fei-Fei Li</i>
    <p>  Despite progress in perceptual tasks such as image classification, computers
still perform poorly on cognitive tasks such as image description and question
answering. Cognition is core to tasks that involve not just recognizing, but
reasoning about our visual world. However, models used to tackle the rich
content in images for cognitive tasks are still being trained using the same
datasets designed for perceptual tasks. To achieve success at cognitive tasks,
models need to understand the interactions and relationships between objects in
an image. When asked &#34;What vehicle is the person riding?&#34;, computers will need
to identify the objects in an image as well as the relationships riding(man,
carriage) and pulling(horse, carriage) in order to answer correctly that &#34;the
person is riding a horse-drawn carriage&#34;.
  In this paper, we present the Visual Genome dataset to enable the modeling of
such relationships. We collect dense annotations of objects, attributes, and
relationships within each image to learn these models. Specifically, our
dataset contains over 100K images where each image has an average of 21
objects, 18 attributes, and 18 pairwise relationships between objects. We
canonicalize the objects, attributes, relationships, and noun phrases in region
descriptions and questions answer pairs to WordNet synsets. Together, these
annotations represent the densest and largest dataset of image descriptions,
objects, attributes, relationships, and question answers.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/jkrause314/status/702914875861786625"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1602.02410" target="_blank">Exploring the Limits of Language Modeling</a> <a href="http://arxiv.org/pdf/1602.02410" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, Yonghui Wu</i>
    <p>  In this work we explore recent advances in Recurrent Neural Networks for
large scale Language Modeling, a task central to language understanding. We
extend current models to deal with two key challenges present in this task:
corpora and vocabulary sizes, and complex, long term structure of language. We
perform an exhaustive study on techniques such as character Convolutional
Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark.
Our best single model significantly improves state-of-the-art perplexity from
51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20),
while an ensemble of models sets a new record by improving perplexity from 41.0
down to 23.7. We also release these models for the NLP and ML community to
study and improve upon.
</p>
    <p>Captured tweets and retweets: 3</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/oriolvinyalsml/status/697078702006472704"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/stanfordnlp/status/697112173420523524"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1601.06759" target="_blank">Pixel Recurrent Neural Networks</a> <a href="http://arxiv.org/pdf/1601.06759" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Aaron van den Oord, Nal Kalchbrenner, Koray Kavukcuoglu</i>
    <p>  Modeling the distribution of natural images is a landmark problem in
unsupervised learning. This task requires an image model that is at once
expressive, tractable and scalable. We present a deep neural network that
sequentially predicts the pixels in an image along the two spatial dimensions.
Our method models the discrete probability of the raw pixel values and encodes
the complete set of dependencies in the image. Architectural novelties include
fast two-dimensional recurrent layers and an effective use of residual
connections in deep recurrent networks. We achieve log-likelihood scores on
natural images that are considerably better than the previous state of the art.
Our main results also provide benchmarks on the diverse ImageNet dataset.
Samples generated from the model appear crisp, varied and globally coherent.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/soumithchintala/status/691990860674797568"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1601.04589" target="_blank">Combining Markov Random Fields and Convolutional Neural Networks for
  Image Synthesis</a> <a href="http://arxiv.org/pdf/1601.04589" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Chuan Li, Michael Wand</i>
    <p>  This paper studies a combination of generative Markov random field (MRF)
models and discriminatively trained deep convolutional neural networks (dCNNs)
for synthesizing 2D images. The generative MRF acts on higher-levels of a dCNN
feature pyramid, controling the image layout at an abstract level. We apply the
method to both photographic and non-photo-realistic (artwork) synthesis tasks.
The MRF regularizer prevents over-excitation artifacts and reduces implausible
feature mixtures common to previous dCNN inversion approaches, permitting
synthezing photographic content with increased visual plausibility. Unlike
standard MRF-based texture synthesis, the combined system can both match and
adapt local features with considerable variability, yielding results far out of
reach of classic generative MRF methods.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/amirsaffari/status/771626411094183936"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1512.05287" target="_blank">A Theoretically Grounded Application of Dropout in Recurrent Neural
  Networks</a> <a href="http://arxiv.org/pdf/1512.05287" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Yarin Gal</i>
    <p>  Recurrent neural networks (RNNs) stand at the forefront of many recent
developments in deep learning. Yet a major difficulty with these models is
their tendency to overfit, with dropout shown to fail when applied to recurrent
layers. Recent results at the intersection of Bayesian modelling and deep
learning offer a Bayesian interpretation of common deep learning techniques
such as dropout. This grounding of dropout in approximate Bayesian inference
suggests an extension of the theoretical results, offering insights into the
use of dropout with RNN models. We apply this new variational inference based
dropout technique in LSTM and GRU models, assessing it on language modelling
and sentiment analysis tasks. The new approach outperforms existing techniques,
and to the best of our knowledge improves on the single model state-of-the-art
in language modelling with the Penn Treebank (73.4 test perplexity). This
extends our arsenal of variational tools in deep learning.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/smerity/status/698649435044057088"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1512.04906" target="_blank">Strategies for Training Large Vocabulary Neural Language Models</a> <a href="http://arxiv.org/pdf/1512.04906" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Welin Chen, David Grangier, Michael Auli</i>
    <p>  Training neural network language models over large vocabularies is still
computationally very costly compared to count-based models such as Kneser-Ney.
At the same time, neural language models are gaining popularity for many
applications such as speech recognition and machine translation whose success
depends on scalability. We present a systematic comparison of strategies to
represent and train large vocabularies, including softmax, hierarchical
softmax, target sampling, noise contrastive estimation and self normalization.
We further extend self normalization to be a proper estimator of likelihood and
introduce an efficient variant of softmax. We evaluate each method on three
popular benchmarks, examining performance on rare words, the speed/accuracy
trade-off and complementarity to Kneser-Ney.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/stanfordnlp/status/697112173420523524"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1511.07122" target="_blank">Multi-Scale Context Aggregation by Dilated Convolutions</a> <a href="http://arxiv.org/pdf/1511.07122" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Fisher Yu, Vladlen Koltun</i>
    <p>  State-of-the-art models for semantic segmentation are based on adaptations of
convolutional networks that had originally been designed for image
classification. However, dense prediction and image classification are
structurally different. In this work, we develop a new convolutional network
module that is specifically designed for dense prediction. The presented module
uses dilated convolutions to systematically aggregate multi-scale contextual
information without losing resolution. The architecture is based on the fact
that dilated convolutions support exponential expansion of the receptive field
without loss of resolution or coverage. We show that the presented context
module increases the accuracy of state-of-the-art semantic segmentation
systems. In addition, we examine the adaptation of image classification
networks to dense prediction and show that simplifying the adapted network can
increase accuracy.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/karpathy/status/729082742328033280"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1511.06361" target="_blank">Order-Embeddings of Images and Language</a> <a href="http://arxiv.org/pdf/1511.06361" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Ivan Vendrov, Ryan Kiros, Sanja Fidler, Raquel Urtasun</i>
    <p>  Hypernymy, textual entailment, and image captioning can be seen as special
cases of a single visual-semantic hierarchy over words, sentences, and images.
In this paper we advocate for explicitly modeling the partial order structure
of this hierarchy. Towards this goal, we introduce a general method for
learning ordered representations, and show how it can be applied to a variety
of tasks involving images and language. We show that the resulting
representations improve performance over current approaches for hypernym
prediction and image-caption retrieval.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/hardmaru/status/772296581202612224"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1511.06279" target="_blank">Neural Programmer-Interpreters</a> <a href="http://arxiv.org/pdf/1511.06279" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Scott Reed, Nando de Freitas</i>
    <p>  We propose the neural programmer-interpreter (NPI): a recurrent and
compositional neural network that learns to represent and execute programs. NPI
has three learnable components: a task-agnostic recurrent core, a persistent
key-value program memory, and domain-specific encoders that enable a single NPI
to operate in multiple perceptually diverse environments with distinct
affordances. By learning to compose lower-level programs to express
higher-level programs, NPI reduces sample complexity and increases
generalization ability compared to sequence-to-sequence LSTMs. The program
memory allows efficient learning of additional tasks by building on existing
programs. NPI can also harness the environment (e.g. a scratch pad with
read-write pointers) to cache intermediate results of computation, lessening
the long-term memory burden on recurrent hidden units. In this work we train
the NPI with fully-supervised execution traces; each program has example
sequences of calls to the immediate subprograms conditioned on the input.
Rather than training on a huge number of relatively weak labels, NPI learns
from a small number of rich examples. We demonstrate the capability of our
model to learn several types of compositional programs: addition, sorting, and
canonicalizing 3D models. Furthermore, a single NPI learns to execute these
programs and all 21 associated subprograms.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/demishassabis/status/721672852110512128"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1511.06114" target="_blank">Multi-task Sequence to Sequence Learning</a> <a href="http://arxiv.org/pdf/1511.06114" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, Lukasz Kaiser</i>
    <p>  Sequence to sequence learning has recently emerged as a new paradigm in
supervised learning. To date, most of its applications focused on only one task
and not much work explored this framework for multiple tasks. This paper
examines three multi-task learning (MTL) settings for sequence to sequence
models: (a) the oneto-many setting - where the encoder is shared between
several tasks such as machine translation and syntactic parsing, (b) the
many-to-one setting - useful when only the decoder can be shared, as in the
case of translation and image caption generation, and (c) the many-to-many
setting - where multiple encoders and decoders are shared, which is the case
with unsupervised objectives and translation. Our results show that training on
a small amount of parsing and image caption data can improve the translation
quality between English and German by up to 1.5 BLEU points over strong
single-task baselines on the WMT benchmarks. Furthermore, we have established a
new state-of-the-art result in constituent parsing with 93.0 F1. Lastly, we
reveal interesting properties of the two unsupervised learning objectives,
autoencoder and skip-thought, in the MTL context: autoencoder helps less in
terms of perplexities but more on BLEU scores compared to skip-thought.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/lmthang/status/704847488893603840"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1511.05756" target="_blank">Image Question Answering using Convolutional Neural Network with Dynamic
  Parameter Prediction</a> <a href="http://arxiv.org/pdf/1511.05756" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Hyeonwoo Noh, Paul Hongsuck Seo, Bohyung Han</i>
    <p>  We tackle image question answering (ImageQA) problem by learning a
convolutional neural network (CNN) with a dynamic parameter layer whose weights
are determined adaptively based on questions. For the adaptive parameter
prediction, we employ a separate parameter prediction network, which consists
of gated recurrent unit (GRU) taking a question as its input and a
fully-connected layer generating a set of candidate weights as its output.
However, it is challenging to construct a parameter prediction network for a
large number of parameters in the fully-connected dynamic parameter layer of
the CNN. We reduce the complexity of this problem by incorporating a hashing
technique, where the candidate weights given by the parameter prediction
network are selected using a predefined hash function to determine individual
weights in the dynamic parameter layer. The proposed network---joint network
with the CNN for ImageQA and the parameter prediction network---is trained
end-to-end through back-propagation, where its weights are initialized using a
pre-trained CNN and GRU. The proposed algorithm illustrates the
state-of-the-art performance on all available public ImageQA benchmarks.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/jnhwkim/status/763891103258910720"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1511.03719" target="_blank">Universum Prescription: Regularization using Unlabeled Data</a> <a href="http://arxiv.org/pdf/1511.03719" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Xiang Zhang, Yann LeCun</i>
    <p>  This paper shows that simply prescribing &#34;none of the above&#34; labels to
unlabeled data has a beneficial regularization effect to supervised learning.
We call it universum prescription by the fact that the prescribed labels cannot
be one of the supervised labels. In spite of its simplicity, universum
prescription obtained competitive results in training deep convolutional
networks for CIFAR-10, CIFAR-100, STL-10 and ImageNet datasets. A qualitative
justification of these approaches using Rademacher complexity is presented. The
effect of a regularization parameter -- probability of sampling from unlabeled
data -- is also studied empirically.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/kastnerkyle/status/734402338681696256"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1511.01844" target="_blank">A note on the evaluation of generative models</a> <a href="http://arxiv.org/pdf/1511.01844" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Lucas Theis, Aäron van den Oord, Matthias Bethge</i>
    <p>  Probabilistic generative models can be used for compression, denoising,
inpainting, texture synthesis, semi-supervised learning, unsupervised feature
learning, and other tasks. Given this wide range of applications, it is not
surprising that a lot of heterogeneity exists in the way these models are
formulated, trained, and evaluated. As a consequence, direct comparison between
models is often difficult. This article reviews mostly known but often
underappreciated properties relating to the evaluation and interpretation of
generative models with a focus on image models. In particular, we show that
three of the currently most commonly used criteria---average log-likelihood,
Parzen window estimates, and visual fidelity of samples---are largely
independent of each other when the data is high-dimensional. Good performance
with respect to one criterion therefore need not imply good performance with
respect to the other criteria. Our results show that extrapolation from one
criterion to another is not warranted and generative models need to be
evaluated directly with respect to the application(s) they were intended for.
In addition, we provide examples demonstrating that Parzen window estimates
should generally be avoided.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/kastnerkyle/status/734186693767618560"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1510.03055" target="_blank">A Diversity-Promoting Objective Function for Neural Conversation Models</a> <a href="http://arxiv.org/pdf/1510.03055" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, Bill Dolan</i>
    <p>  Sequence-to-sequence neural network models for generation of conversational
responses tend to generate safe, commonplace responses (e.g., &#34;I don&#39;t know&#34;)
regardless of the input. We suggest that the traditional objective function,
i.e., the likelihood of output (response) given input (message) is unsuited to
response generation tasks. Instead we propose using Maximum Mutual Information
(MMI) as the objective function in neural models. Experimental results
demonstrate that the proposed MMI models produce more diverse, interesting, and
appropriate responses, yielding substantive gains in BLEU scores on two
conversational datasets and in human evaluations.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/stanfordnlp/status/707588046745509888"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1508.04025" target="_blank">Effective Approaches to Attention-based Neural Machine Translation</a> <a href="http://arxiv.org/pdf/1508.04025" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Minh-Thang Luong, Hieu Pham, Christopher D. Manning</i>
    <p>  An attentional mechanism has lately been used to improve neural machine
translation (NMT) by selectively focusing on parts of the source sentence
during translation. However, there has been little work exploring useful
architectures for attention-based NMT. This paper examines two simple and
effective classes of attentional mechanism: a global approach which always
attends to all source words and a local one that only looks at a subset of
source words at a time. We demonstrate the effectiveness of both approaches
over the WMT translation tasks between English and German in both directions.
With local attention, we achieve a significant gain of 5.0 BLEU points over
non-attentional systems which already incorporate known techniques such as
dropout. Our ensemble model using different attention architectures has
established a new state-of-the-art result in the WMT&#39;15 English to German
translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over
the existing best system backed by NMT and an n-gram reranker.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/kchonyc/status/753067086746906624"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1506.08909" target="_blank">The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured
  Multi-Turn Dialogue Systems</a> <a href="http://arxiv.org/pdf/1506.08909" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Ryan Lowe, Nissan Pow, Iulian Serban, Joelle Pineau</i>
    <p>  This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost
1 million multi-turn dialogues, with a total of over 7 million utterances and
100 million words. This provides a unique resource for research into building
dialogue managers based on neural language models that can make use of large
amounts of unlabeled data. The dataset has both the multi-turn property of
conversations in the Dialog State Tracking Challenge datasets, and the
unstructured nature of interactions from microblog services such as Twitter. We
also describe two neural learning architectures suitable for analyzing this
dataset, and provide benchmark performance on the task of selecting the best
next response.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/nandodf/status/745972271467081728"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1506.07365" target="_blank">Embed to Control: A Locally Linear Latent Dynamics Model for Control
  from Raw Images</a> <a href="http://arxiv.org/pdf/1506.07365" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, Martin Riedmiller</i>
    <p>  We introduce Embed to Control (E2C), a method for model learning and control
of non-linear dynamical systems from raw pixel images. E2C consists of a deep
generative model, belonging to the family of variational autoencoders, that
learns to generate image trajectories from a latent space in which the dynamics
is constrained to be locally linear. Our model is derived directly from an
optimal control formulation in latent space, supports long-term prediction of
image sequences and exhibits strong performance on a variety of complex control
problems.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/mat_kelcey/status/763462515732865024"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1506.07285" target="_blank">Ask Me Anything: Dynamic Memory Networks for Natural Language Processing</a> <a href="http://arxiv.org/pdf/1506.07285" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, Richard Socher</i>
    <p>  Most tasks in natural language processing can be cast into question answering
(QA) problems over language input. We introduce the dynamic memory network
(DMN), a neural network architecture which processes input sequences and
questions, forms episodic memories, and generates relevant answers. Questions
trigger an iterative attention process which allows the model to condition its
attention on the inputs and the result of previous iterations. These results
are then reasoned over in a hierarchical recurrent sequence model to generate
answers. The DMN can be trained end-to-end and obtains state-of-the-art results
on several types of tasks and datasets: question answering (Facebook&#39;s bAbI
dataset), text classification for sentiment analysis (Stanford Sentiment
Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The
training for these different tasks relies exclusively on trained word vector
representations and input-question-answer triplets.
</p>
    <p>Captured tweets and retweets: 9</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/richardsocher/status/743214026264444928"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/richardsocher/status/724369376925749248"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/stephenpiment/status/702735139793539072"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/richardsocher/status/646739800012271616"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/richardsocher/status/646738608087367681"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/richardsocher/status/618565826082271232"></a></blockquote></center>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/stephenpiment/status/614433331955003393"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1506.01070" target="_blank">Do Multi-Sense Embeddings Improve Natural Language Understanding?</a> <a href="http://arxiv.org/pdf/1506.01070" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Jiwei Li, Dan Jurafsky</i>
    <p>  Learning a distinct representation for each sense of an ambiguous word could
lead to more powerful and fine-grained models of vector-space representations.
Yet while `multi-sense&#39; methods have been proposed and tested on artificial
word-similarity tasks, we don&#39;t know if they improve real natural language
understanding tasks. In this paper we introduce a multi-sense embedding model
based on Chinese Restaurant Processes that achieves state of the art
performance on matching human word similarity judgments, and propose a
pipelined architecture for incorporating multi-sense embeddings into language
understanding.
  We then test the performance of our model on part-of-speech tagging, named
entity recognition, sentiment analysis, semantic relation identification and
semantic relatedness, controlling for embedding dimensionality. We find that
multi-sense embeddings do improve performance on some tasks (part-of-speech
tagging, semantic relation identification, semantic relatedness) but not on
others (named entity recognition, various forms of sentiment analysis). We
discuss how these differences may be caused by the different role of word sense
information in each of the tasks. The results highlight the importance of
testing embedding models in real applications.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/david__jurgens/status/773303283247046656"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1506.01066" target="_blank">Visualizing and Understanding Neural Models in NLP</a> <a href="http://arxiv.org/pdf/1506.01066" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Jiwei Li, Xinlei Chen, Eduard Hovy, Dan Jurafsky</i>
    <p>  While neural networks have been successfully applied to many NLP tasks the
resulting vector-based models are very difficult to interpret. For example it&#39;s
not clear how they achieve {\em compositionality}, building sentence meaning
from the meanings of words and phrases. In this paper we describe four
strategies for visualizing compositionality in neural models for NLP, inspired
by similar work in computer vision. We first plot unit values to visualize
compositionality of negation, intensification, and concessive clauses, allow us
to see well-known markedness asymmetries in negation. We then introduce three
simple and straightforward methods for visualizing a unit&#39;s {\em salience}, the
amount it contributes to the final composed meaning: (1) gradient
back-propagation, (2) the variance of a token from the average word node, (3)
LSTM-style gates that measure information flow. We test our methods on
sentiment using simple recurrent nets and LSTMs. Our general-purpose methods
may have wide applications for understanding compositionality and other
semantic properties of deep networks , and also shed light on why LSTMs
outperform simple recurrent nets,
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/stanfordnlp/status/707588046745509888"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1505.01504" target="_blank">A Fixed-Size Encoding Method for Variable-Length Sequences with its
  Application to Neural Network Language Models</a> <a href="http://arxiv.org/pdf/1505.01504" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Shiliang Zhang, Hui Jiang, Mingbin Xu, Junfeng Hou, Lirong Dai</i>
    <p>  In this paper, we propose the new fixed-size ordinally-forgetting encoding
(FOFE) method, which can almost uniquely encode any variable-length sequence of
words into a fixed-size representation. FOFE can model the word order in a
sequence using a simple ordinally-forgetting mechanism according to the
positions of words. In this work, we have applied FOFE to feedforward neural
network language models (FNN-LMs). Experimental results have shown that without
using any recurrent feedbacks, FOFE based FNN-LMs can significantly outperform
not only the standard fixed-input FNN-LMs but also the popular RNN-LMs.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/richardsocher/status/597632520872595456"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1503.04069" target="_blank">LSTM: A Search Space Odyssey</a> <a href="http://arxiv.org/pdf/1503.04069" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Klaus Greff, Rupesh Kumar Srivastava, Jan Koutník, Bas R. Steunebrink, Jürgen Schmidhuber</i>
    <p>  Several variants of the Long Short-Term Memory (LSTM) architecture for
recurrent neural networks have been proposed since its inception in 1995. In
recent years, these networks have become the state-of-the-art models for a
variety of machine learning problems. This has led to a renewed interest in
understanding the role and utility of various computational components of
typical LSTM variants. In this paper, we present the first large-scale analysis
of eight LSTM variants on three representative tasks: speech recognition,
handwriting recognition, and polyphonic music modeling. The hyperparameters of
all LSTM variants for each task were optimized separately using random search
and their importance was assessed using the powerful fANOVA framework. In
total, we summarize the results of 5400 experimental runs (about 15 years of
CPU time), which makes our study the largest of its kind on LSTM networks. Our
results show that none of the variants can improve upon the standard LSTM
architecture significantly, and demonstrate the forget gate and the output
activation function to be its most critical components. We further observe that
the studied hyperparameters are virtually independent and derive guidelines for
their efficient adjustment.
</p>
    <p>Captured tweets and retweets: 2</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/andrpem/status/769728735411003392"></a></blockquote></center>
    
  </div>
  <hr />
  
  <div>
    <h4><a href="http://arxiv.org/abs/1502.04390" target="_blank">Equilibrated adaptive learning rates for non-convex optimization</a> <a href="http://arxiv.org/pdf/1502.04390" name="PDF" target="_blank"><span class="glyphicon glyphicon-file" aria-hidden="true"></span></a></h4>
    <i>Yann N. Dauphin, Harm de Vries, Yoshua Bengio</i>
    <p>  Parameter-specific adaptive learning rate methods are computationally
efficient ways to reduce the ill-conditioning problems encountered when
training large deep networks. Following recent work that strongly suggests that
most of the critical points encountered when training such networks are saddle
points, we find how considering the presence of negative eigenvalues of the
Hessian could help us design better suited adaptive learning rate schemes. We
show that the popular Jacobi preconditioner has undesirable behavior in the
presence of both positive and negative curvature, and present theoretical and
empirical evidence that the so-called equilibration preconditioner is
comparatively better suited to non-convex problems. We introduce a novel
adaptive learning rate scheme, called ESGD, based on the equilibration
preconditioner. Our experiments show that ESGD performs as well or better than
RMSProp in terms of convergence speed, always clearly improving over plain
stochastic gradient descent.
</p>
    <p>Captured tweets and retweets: 1</p>
    
    <center><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><a href="https://twitter.com/chrmanning/status/691801060441223168"></a></blockquote></center>
    
  </div>
  <hr />
  

  </div>
</body>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
</html>